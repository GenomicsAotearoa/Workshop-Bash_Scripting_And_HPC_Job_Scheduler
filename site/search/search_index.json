{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Bash Scripting and HPC Scheduler \u00b6 This as an Introductory level workshop on Bash Scripting and HPC Job Scheduler, Slurm . Prerequisites \u00b6 Familiarity with terminal and basic linux commands Intermediate level knowledge on Molecular Biology and Genetics Some of the things we won't cover in this workshop \u00b6 Domain specific concepts in Genetics, Genomics and DNA Sequencing Variant Calling RNA sequencing and data analysis Content \u00b6 Designing a Variant Calling Workflow Automating a Variant Calling Workflow RNA-seq Mapping And Count Data Workflow Introduction to HPC Working with Job Scheduler Supplementary #1 Supplementary #2 Supplementary #3 Setup \u00b6 Workshop material is designed to run on NeSI Mahuika cluster via Jupyter. Instructions on how to Set/Reset Authentication factors to access NeSI Services and Jupyter Login instructions can be found here","title":"Home"},{"location":"#introduction-to-bash-scripting-and-hpc-scheduler","text":"This as an Introductory level workshop on Bash Scripting and HPC Job Scheduler, Slurm .","title":"Introduction to Bash Scripting and HPC Scheduler"},{"location":"#prerequisites","text":"Familiarity with terminal and basic linux commands Intermediate level knowledge on Molecular Biology and Genetics","title":"Prerequisites"},{"location":"#some-of-the-things-we-wont-cover-in-this-workshop","text":"Domain specific concepts in Genetics, Genomics and DNA Sequencing Variant Calling RNA sequencing and data analysis","title":"Some of the things we won't cover in this workshop"},{"location":"#content","text":"Designing a Variant Calling Workflow Automating a Variant Calling Workflow RNA-seq Mapping And Count Data Workflow Introduction to HPC Working with Job Scheduler Supplementary #1 Supplementary #2 Supplementary #3","title":"Content"},{"location":"#setup","text":"Workshop material is designed to run on NeSI Mahuika cluster via Jupyter. Instructions on how to Set/Reset Authentication factors to access NeSI Services and Jupyter Login instructions can be found here","title":"Setup"},{"location":"1_DesigningVariantC/","text":"Variant Calling Workflow \u00b6 This material is extracted from the Genomics Data Carpentry Lesson Aim To understand the steps to perform variant calling then overall put all these steps into a script. Objectives and overall workflow Understand and perform the steps involved in variant calling. Describe the types of data formats encountered during variant calling. Use command line tools to perform variant calling. Assumptions You have already performed trimming and filtering of your reads and saved in a directory called trimmed_reads. You have a reference genome saved in a directory called ref_genome . In this workshop, we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are: cd ~ pwd Checking to make sure we have the directory and files for the workshop. ls Hint If you do not have the workshop directory, you can copy it using the command: cp -r /nesi/project/nesi02659/scripting_workshop/ ~ cd scripting_workshop/variant_calling $ ls ref_genome trimmed_reads Alignment to a reference genome \u00b6 First we need to create directories for the results that will be generated as part of this workflow. We can do this in a single line of code, because mkdir can accept multiple new directory names as input. mkdir -p results/sam results/bam results/bcf results/vcf Index the reference genome \u00b6 Our first step is to index the reference genome for use by BWA. Indexing allows the aligner to quickly find potential alignment sites for query sequences in a genome, which saves time during alignment. Indexing the reference only has to be run once. The only reason you would want to create a new index is if you are working with a different reference genome or you are using a different tool for alignment. Since we are working on the NeSI HPC, we need to search and load the package before we start using it. - More on packages will be discussed in the HPC and Slurm section Search module spider bwa and then load BWA module. module purge module load BWA/0.7.17-GCC-9.2.0 indexing the genome bwa index ref_genome/ecoli_rel606.fasta Output [ bwa_index ] Pack FASTA... 0 .03 sec [ bwa_index ] Construct BWT for the packed sequence... [ bwa_index ] 1 .04 seconds elapse. [ bwa_index ] Update BWT... 0 .03 sec [ bwa_index ] Pack forward-only FASTA... 0 .02 sec [ bwa_index ] Construct SA from BWT and Occ... 0 .57 sec [ main ] Version: 0 .7.17-r1188 [ main ] CMD: bwa index ref_genome/ecoli_rel606.fasta [ main ] Real time: 2 .462 sec ; CPU: 1 .702 sec Align reads to reference genome \u00b6 The alignment process consists of choosing an appropriate reference genome to map our reads against and then deciding on an aligner. We will use the BWA-MEM algorithm, which is the latest and is generally recommended for high-quality queries as it is faster and more accurate. We are going to start by aligning the reads from just one of the samples in our dataset (SRR2584866). $ bwa mem ref_genome/ecoli_rel606.fasta trimmed_reads/SRR2584866_1.trim.sub.fastq trimmed_reads/SRR2584866_2.trim.sub.fastq > results/sam/SRR2584866.aligned.sam [ M::bwa_idx_load_from_disk ] read 0 ALT contigs [ M::process ] read 77446 sequences ( 10000033 bp ) ... [ M::process ] read 77296 sequences ( 10000182 bp ) ... [ M::mem_pestat ] # candidate unique pairs for (FF, FR, RF, RR): (48, 36728, 21, 61) [ M::mem_pestat ] analyzing insert size distribution for orientation FF... [ M::mem_pestat ] ( 25 , 50 , 75 ) percentile: ( 420 , 660 , 1774 ) [ M::mem_pestat ] low and high boundaries for computing mean and std.dev: ( 1 , 4482 ) ..... $ ls results/sam/ SRR2584866.aligned.sam SAM/BAM format \u00b6 The SAM file, is a tab-delimited text file that contains information for each individual read and its alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification. The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file. We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (-S) and to output BAM format (-b): We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (-S) and to output BAM format (-b): module load SAMtools/1.13-GCC-9.2.0 samtools view -S -b results/sam/SRR2584866.aligned.sam > results/bam/SRR2584866.aligned.bam Sort BAM file by coordinates \u00b6 Next we sort the BAM file using the sort command from samtools. -o tells the command where to write the output. samtools sort -o results/bam/SRR2584866.aligned.sorted.bam results/bam/SRR2584866.aligned.bam hint: SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input. You can use samtools to learn more about this bam file as well. samtools flagstat results/bam/SRR2584866.aligned.sorted.bam Variant calling \u00b6 A variant call is a conclusion that there is a nucleotide difference vs. some reference at a given position in an individual genome or transcriptome, often referred to as a Single Nucleotide Variant (SNV). The call is usually accompanied by an estimate of variant frequency and some measure of confidence. Similar to other steps in this workflow, there are a number of tools available for variant calling. In this workshop we will be using bcftools , but there are a few things we need to do before actually calling the variants. Step 1: Calculate the read coverage of positions in the genome \u00b6 Do the first pass on variant calling by counting read coverage with bcftools . We will use the command mpileup. The flag -O b tells bcftools to generate a bcf format output file, -o specifies where to write the output file, and -f flags the path to the reference genome: module load BCFtools/1.13-GCC-9.2.0 $ bcftools mpileup -O b -o results/bcf/SRR2584866_raw.bcf -f ref_genome/ecoli_rel606.fasta results/bam/SRR2584866.aligned.sorted.bam [ mpileup ] 1 samples in 1 input files [ mpileup ] maximum number of reads per input file set to -d 250 We have now generated a file with coverage information for every base. Step 2: Detect the single nucleotide variants (SNVs) \u00b6 Identify SNVs using bcftools call. We have to specify ploidy with the flag --ploidy , which is one for the haploid E. coli. -m allows for multiallelic and rare-variant calling, -v tells the program to output variant sites only (not every site in the genome), and -o specifies where to write the output file: bcftools call --ploidy 1 -m -v -o results/vcf/SRR2584866_variants.vcf results/bcf/SRR2584866_raw.bcf Step 3: Filter and report the SNV variants in variant calling format (VCF) \u00b6 Filter the SNVs for the final output in VCF format, using vcfutils.pl : vcfutils.pl varFilter results/vcf/SRR2584866_variants.vcf > results/vcf/SRR2584866_final_variants.vcf Explore the VCF format: \u00b6 At this stage you can use various tools to analyse the vcf file. Exploring the vcf is beyond the scope of this workshop. Now we are ready for the Next Lesson to put all these commands in a script.","title":"Variant Calling Workflow"},{"location":"1_DesigningVariantC/#variant-calling-workflow","text":"This material is extracted from the Genomics Data Carpentry Lesson Aim To understand the steps to perform variant calling then overall put all these steps into a script. Objectives and overall workflow Understand and perform the steps involved in variant calling. Describe the types of data formats encountered during variant calling. Use command line tools to perform variant calling. Assumptions You have already performed trimming and filtering of your reads and saved in a directory called trimmed_reads. You have a reference genome saved in a directory called ref_genome . In this workshop, we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are: cd ~ pwd Checking to make sure we have the directory and files for the workshop. ls Hint If you do not have the workshop directory, you can copy it using the command: cp -r /nesi/project/nesi02659/scripting_workshop/ ~ cd scripting_workshop/variant_calling $ ls ref_genome trimmed_reads","title":"Variant Calling Workflow"},{"location":"1_DesigningVariantC/#alignment-to-a-reference-genome","text":"First we need to create directories for the results that will be generated as part of this workflow. We can do this in a single line of code, because mkdir can accept multiple new directory names as input. mkdir -p results/sam results/bam results/bcf results/vcf","title":"Alignment to a reference genome"},{"location":"1_DesigningVariantC/#index-the-reference-genome","text":"Our first step is to index the reference genome for use by BWA. Indexing allows the aligner to quickly find potential alignment sites for query sequences in a genome, which saves time during alignment. Indexing the reference only has to be run once. The only reason you would want to create a new index is if you are working with a different reference genome or you are using a different tool for alignment. Since we are working on the NeSI HPC, we need to search and load the package before we start using it. - More on packages will be discussed in the HPC and Slurm section Search module spider bwa and then load BWA module. module purge module load BWA/0.7.17-GCC-9.2.0 indexing the genome bwa index ref_genome/ecoli_rel606.fasta Output [ bwa_index ] Pack FASTA... 0 .03 sec [ bwa_index ] Construct BWT for the packed sequence... [ bwa_index ] 1 .04 seconds elapse. [ bwa_index ] Update BWT... 0 .03 sec [ bwa_index ] Pack forward-only FASTA... 0 .02 sec [ bwa_index ] Construct SA from BWT and Occ... 0 .57 sec [ main ] Version: 0 .7.17-r1188 [ main ] CMD: bwa index ref_genome/ecoli_rel606.fasta [ main ] Real time: 2 .462 sec ; CPU: 1 .702 sec","title":"Index the reference genome"},{"location":"1_DesigningVariantC/#align-reads-to-reference-genome","text":"The alignment process consists of choosing an appropriate reference genome to map our reads against and then deciding on an aligner. We will use the BWA-MEM algorithm, which is the latest and is generally recommended for high-quality queries as it is faster and more accurate. We are going to start by aligning the reads from just one of the samples in our dataset (SRR2584866). $ bwa mem ref_genome/ecoli_rel606.fasta trimmed_reads/SRR2584866_1.trim.sub.fastq trimmed_reads/SRR2584866_2.trim.sub.fastq > results/sam/SRR2584866.aligned.sam [ M::bwa_idx_load_from_disk ] read 0 ALT contigs [ M::process ] read 77446 sequences ( 10000033 bp ) ... [ M::process ] read 77296 sequences ( 10000182 bp ) ... [ M::mem_pestat ] # candidate unique pairs for (FF, FR, RF, RR): (48, 36728, 21, 61) [ M::mem_pestat ] analyzing insert size distribution for orientation FF... [ M::mem_pestat ] ( 25 , 50 , 75 ) percentile: ( 420 , 660 , 1774 ) [ M::mem_pestat ] low and high boundaries for computing mean and std.dev: ( 1 , 4482 ) ..... $ ls results/sam/ SRR2584866.aligned.sam","title":"Align reads to reference genome"},{"location":"1_DesigningVariantC/#sambam-format","text":"The SAM file, is a tab-delimited text file that contains information for each individual read and its alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification. The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file. We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (-S) and to output BAM format (-b): We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (-S) and to output BAM format (-b): module load SAMtools/1.13-GCC-9.2.0 samtools view -S -b results/sam/SRR2584866.aligned.sam > results/bam/SRR2584866.aligned.bam","title":"SAM/BAM format"},{"location":"1_DesigningVariantC/#sort-bam-file-by-coordinates","text":"Next we sort the BAM file using the sort command from samtools. -o tells the command where to write the output. samtools sort -o results/bam/SRR2584866.aligned.sorted.bam results/bam/SRR2584866.aligned.bam hint: SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input. You can use samtools to learn more about this bam file as well. samtools flagstat results/bam/SRR2584866.aligned.sorted.bam","title":"Sort BAM file by coordinates"},{"location":"1_DesigningVariantC/#variant-calling","text":"A variant call is a conclusion that there is a nucleotide difference vs. some reference at a given position in an individual genome or transcriptome, often referred to as a Single Nucleotide Variant (SNV). The call is usually accompanied by an estimate of variant frequency and some measure of confidence. Similar to other steps in this workflow, there are a number of tools available for variant calling. In this workshop we will be using bcftools , but there are a few things we need to do before actually calling the variants.","title":"Variant calling"},{"location":"1_DesigningVariantC/#step-1-calculate-the-read-coverage-of-positions-in-the-genome","text":"Do the first pass on variant calling by counting read coverage with bcftools . We will use the command mpileup. The flag -O b tells bcftools to generate a bcf format output file, -o specifies where to write the output file, and -f flags the path to the reference genome: module load BCFtools/1.13-GCC-9.2.0 $ bcftools mpileup -O b -o results/bcf/SRR2584866_raw.bcf -f ref_genome/ecoli_rel606.fasta results/bam/SRR2584866.aligned.sorted.bam [ mpileup ] 1 samples in 1 input files [ mpileup ] maximum number of reads per input file set to -d 250 We have now generated a file with coverage information for every base.","title":"Step 1: Calculate the read coverage of positions in the genome"},{"location":"1_DesigningVariantC/#step-2-detect-the-single-nucleotide-variants-snvs","text":"Identify SNVs using bcftools call. We have to specify ploidy with the flag --ploidy , which is one for the haploid E. coli. -m allows for multiallelic and rare-variant calling, -v tells the program to output variant sites only (not every site in the genome), and -o specifies where to write the output file: bcftools call --ploidy 1 -m -v -o results/vcf/SRR2584866_variants.vcf results/bcf/SRR2584866_raw.bcf","title":"Step 2: Detect the single nucleotide variants (SNVs)"},{"location":"1_DesigningVariantC/#step-3-filter-and-report-the-snv-variants-in-variant-calling-format-vcf","text":"Filter the SNVs for the final output in VCF format, using vcfutils.pl : vcfutils.pl varFilter results/vcf/SRR2584866_variants.vcf > results/vcf/SRR2584866_final_variants.vcf","title":"Step 3: Filter and report the SNV variants in variant calling format (VCF)"},{"location":"1_DesigningVariantC/#explore-the-vcf-format","text":"At this stage you can use various tools to analyse the vcf file. Exploring the vcf is beyond the scope of this workshop. Now we are ready for the Next Lesson to put all these commands in a script.","title":"Explore the VCF format:"},{"location":"2_AutomaticVariantC/","text":"Automating a Variant Calling Workflow \u00b6 Aim Put all the steps from the previous lesson into a script. Variant calling workflow \u00b6 Remember our variant calling workflow has the following steps: Index the reference genome for use by bwa and samtools. Align reads to reference genome. Convert the format of the alignment to sorted BAM, with some intermediate steps. Calculate the read coverage of positions in the genome. Detect the single nucleotide variants (SNVs). Filter and report the SNVs in VCF (variant calling format). Let's start with creating a new directory as our script working space and copy all the required resources. $ pwd /home/ [ Your_Username ] /scripting_workshop $ mkdir script_workspace $ cd script_workspace $ cp -r /nesi/project/nesi02659/scripting_workshop/variant_calling/* . $ ls ref_genome trimmed_reads Now we are ready to start building the script. $ nano variant_calling.sh In the text editor, type the commands #!/bin/bash # Jane Doe # 05 March 2022 # This script runs the variant calling pipeline from mapping to vcf. set -e # Load all the required modules module purge module load BWA/0.7.17-GCC-9.2.0 module load SAMtools/1.13-GCC-9.2.0 module load BCFtools/1.13-GCC-9.2.0 # create the results directories mkdir -p results/sam results/bam results/bcf results/vcf # indexing the genome genome = ref_genome/ecoli_rel606.fasta bwa index $genome # create a loop that map reads to the genome, sort the bam files and call variants for fq1 in trimmed_reads/*_1.trim.sub.fastq do echo \"working with file $fq1 \" base = $( basename $fq1 _1.trim.sub.fastq ) echo \"base name is $base \" # setting the variables fq1 = trimmed_reads/ ${ base } _1.trim.sub.fastq fq2 = trimmed_reads/ ${ base } _2.trim.sub.fastq sam = results/sam/ ${ base } .aligned.sam bam = results/bam/ ${ base } .aligned.bam sorted_bam = results/bam/ ${ base } .aligned.sorted.bam raw_bcf = results/bcf/ ${ base } _raw.bcf variants = results/vcf/ ${ base } _variants.vcf final_variants = results/vcf/ ${ base } _final_variants.vcf # running the analysis steps bwa mem $genome $fq1 $fq2 > $sam samtools view -S -b $sam > $bam samtools sort -o $sorted_bam $bam samtools index $sorted_bam bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam bcftools call --ploidy 1 -m -v -o $variants $raw_bcf vcfutils.pl varFilter $variants > $final_variants done Running the script $ bash ./variant_calling.sh This should take about 10 minutes. Note: The way the script is written means we have to indicate which program to use whenever we are running it. So to run without calling bash, we can change the script permissions. $ ls -l variant_calling.sh -rw-rw-r-- 1 fayfa80p fayfa80p 1401 Mar 5 22 :29 variant_calling.sh $ chmod u+x variant_calling.sh $ ls -l variant_calling.sh -rwxrw-r-- 1 fayfa80p fayfa80p 1401 Mar 5 22 :29 variant_calling.sh # note colour change on the script filename Now we can execute the script without calling bash $ ./variant_calling.sh In the Next Lesson we will now prepare the script to run on the HPC environment","title":"Automating a Variant Calling Workflow"},{"location":"2_AutomaticVariantC/#automating-a-variant-calling-workflow","text":"Aim Put all the steps from the previous lesson into a script.","title":"Automating a Variant Calling Workflow"},{"location":"2_AutomaticVariantC/#variant-calling-workflow","text":"Remember our variant calling workflow has the following steps: Index the reference genome for use by bwa and samtools. Align reads to reference genome. Convert the format of the alignment to sorted BAM, with some intermediate steps. Calculate the read coverage of positions in the genome. Detect the single nucleotide variants (SNVs). Filter and report the SNVs in VCF (variant calling format). Let's start with creating a new directory as our script working space and copy all the required resources. $ pwd /home/ [ Your_Username ] /scripting_workshop $ mkdir script_workspace $ cd script_workspace $ cp -r /nesi/project/nesi02659/scripting_workshop/variant_calling/* . $ ls ref_genome trimmed_reads Now we are ready to start building the script. $ nano variant_calling.sh In the text editor, type the commands #!/bin/bash # Jane Doe # 05 March 2022 # This script runs the variant calling pipeline from mapping to vcf. set -e # Load all the required modules module purge module load BWA/0.7.17-GCC-9.2.0 module load SAMtools/1.13-GCC-9.2.0 module load BCFtools/1.13-GCC-9.2.0 # create the results directories mkdir -p results/sam results/bam results/bcf results/vcf # indexing the genome genome = ref_genome/ecoli_rel606.fasta bwa index $genome # create a loop that map reads to the genome, sort the bam files and call variants for fq1 in trimmed_reads/*_1.trim.sub.fastq do echo \"working with file $fq1 \" base = $( basename $fq1 _1.trim.sub.fastq ) echo \"base name is $base \" # setting the variables fq1 = trimmed_reads/ ${ base } _1.trim.sub.fastq fq2 = trimmed_reads/ ${ base } _2.trim.sub.fastq sam = results/sam/ ${ base } .aligned.sam bam = results/bam/ ${ base } .aligned.bam sorted_bam = results/bam/ ${ base } .aligned.sorted.bam raw_bcf = results/bcf/ ${ base } _raw.bcf variants = results/vcf/ ${ base } _variants.vcf final_variants = results/vcf/ ${ base } _final_variants.vcf # running the analysis steps bwa mem $genome $fq1 $fq2 > $sam samtools view -S -b $sam > $bam samtools sort -o $sorted_bam $bam samtools index $sorted_bam bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam bcftools call --ploidy 1 -m -v -o $variants $raw_bcf vcfutils.pl varFilter $variants > $final_variants done Running the script $ bash ./variant_calling.sh This should take about 10 minutes. Note: The way the script is written means we have to indicate which program to use whenever we are running it. So to run without calling bash, we can change the script permissions. $ ls -l variant_calling.sh -rw-rw-r-- 1 fayfa80p fayfa80p 1401 Mar 5 22 :29 variant_calling.sh $ chmod u+x variant_calling.sh $ ls -l variant_calling.sh -rwxrw-r-- 1 fayfa80p fayfa80p 1401 Mar 5 22 :29 variant_calling.sh # note colour change on the script filename Now we can execute the script without calling bash $ ./variant_calling.sh In the Next Lesson we will now prepare the script to run on the HPC environment","title":"Variant calling workflow"},{"location":"3_RNAseq/","text":"RNA-seq Mapping And Count Data Workflow \u00b6 This material is extracted from the RNA-seq workshop Lesson Aim To develop a pipeline that does mapping and count the number of reads that mapped then overall put all these steps into a script. Objectives and overall workflow Understand and perform the steps involved in RNA-seq mapping and read count. Use command line tools to run the pipeline. Assumptions \u00b6 You have already performed trimming and filtering of your reads and saved in a directory called trimmed_reads. You have a reference genome saved in a directory called ref_genome. In this workshop, we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are: $ cd ~/scripting_workshop $ pwd /home/ [ your_username ] /scripting_workshop # good I am ready to work Checking to make sure we have the directory and files for the workshop. $ ls rna_seq variant_calling scheduler hint : If you do not have the workshop directory, you can copy it using the command: cp -r /nesi/project/nesi02659/scripting_workshop/ ~ $ cd rna_seq $ ls ref_genome trimmed_reads Alignment to a reference genome \u00b6 RNA-seq generate gene expression information by quantifying the number of transcripts (per gene) in a sample. This is acompished by counting the number of transcripts that have been sequenced - the more active a gene is, the more transcripts will be in a sample, and the more reads will be generated from that transcript. For RNA-seq, we need to align or map each read back to the genome, to see which gene produced it. - Highly expressed genes will generate lots of transcripts, so there will be lots of reads that map back to the position of that transcript in the genome. - The per-gene data we work with in an RNA-seq experiment are counts: the number of reads from each sample that originated from that gene. Preparation of the genome \u00b6 To be able to map (align) sequencing reads on the genome, the genome needs to be indexed first. In this workshop we will use HISAT2 . $ cd ~/scripting_workshop/rna_seq/ref_genome #to list what is in your directory: $ ls ~/scripting_workshop/rna_seq/ref_genome Saccharomyces_cerevisiae.R64-1-1.99.gtf Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa $ module load HISAT2/2.2.0-gimkl-2020a # index file: $ hisat2-build -p 4 -f Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa Saccharomyces_cerevisiae.R64-1-1.dna.toplevel #list what is in the directory: $ ls ~/scripting_workshop/rna_seq/ref_genome Saccharomyces_cerevisiae.R64-1-1.99.gtf Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.4.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.8.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.1.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.5.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.2.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.6.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.3.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.7.ht2 Arguments: * -p number of threads * -f fasta file How many files were created during the indexing process? Alignment on the genome \u00b6 Now that the genome is prepared. Sequencing reads can be aligned. Information required: Where the sequence information is stored (e.g. fastq files ...) ? What kind of sequencing: Single End or Paired end ? Where are stored the indexes and the genome? Where will the mapping files be stored? Now, lets move one folder up (into the rna_seq folder): $ cd .. $ ls ref_genome trimmed_reads Let's map one of our sample to the reference genome $ pwd /home/ [ Your_Username ] /scripting_workshop/rna_seq/ $ mkdir Mapping $ ls ref_genome Mapping trimmed_reads let's use a for loop to process our samples: $ cd trimmed_reads $ ls SRR014335-chr1.fastq SRR014336-chr1.fastq SRR014337-chr1.fastq SRR014339-chr1.fastq SRR014340-chr1.fastq SRR014341-chr1.fastq $ for filename in * > do > base = $( basename ${ filename } .fastq ) > hisat2 -p 4 -x ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S ../Mapping/ ${ base } .sam --summary-file ../Mapping/ ${ base } _summary.txt > done Arguments: * -x The basename of the index for the reference genome. * -U Comma-separated list of files containing unpaired reads to be aligned * -S File to write SAM alignments to. By default, alignments are written to the \u201cstandard out\u201d or \u201cstdout\u201d filehandle Now we can explore our SAM files. $ cd ../Mapping $ ls SRR014335-chr1.sam SRR014336-chr1_summary.txt SRR014339-chr1.sam SRR014340-chr1_summary.txt SRR014335-chr1_summary.txt SRR014337-chr1.sam SRR014339-chr1_summary.txt SRR014341-chr1.sam SRR014336-chr1.sam SRR014337-chr1_summary.txt SRR014340-chr1.sam SRR014341-chr1_summary.txt Converting SAM files to BAM files \u00b6 The SAM file, is a tab-delimited text file that contains information for each individual read and its alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification. The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file. A quick look into the sam file \u00b6 $ less SRR014335-chr1.sam The file begins with a header, which is optional. The header is used to describe the source of data, reference sequence, method of alignment, etc., this will change depending on the aligner being used. Following the header is the alignment section. Each line that follows corresponds to alignment information for a single read. Each alignment line has 11 mandatory fields for essential mapping information and a variable number of other fields for aligner specific information. An example entry from a SAM file is displayed below with the different fields highlighted. We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format ( -S ) and to output BAM format ( -b ): $ module load SAMtools/1.10-GCC-9.2.0 $ for filename in *.sam > do > base = $( basename ${ filename } .sam ) > samtools view -S -b ${ filename } -o ${ base } .bam > done $ ls SRR014335-chr1.bam SRR014336-chr1.bam SRR014337-chr1.bam SRR014339-chr1.bam SRR014340-chr1.bam SRR014341-chr1.bam SRR014335-chr1.sam SRR014336-chr1.sam SRR014337-chr1.sam SRR014339-chr1.sam SRR014340-chr1.sam SRR014341-chr1.sam Next we sort the BAM file using the sort command from samtools. -o tells the command where to write the output. Note SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input.** $ for filename in *.bam > do > base = $( basename ${ filename } .bam ) > samtools sort -o ${ base } _sorted.bam ${ filename } > done We can use samtools to learn more about the bam file as well. Some stats on your mapping: \u00b6 $ samtools flagstat SRR014335-chr1_sorted.bam 156984 + 0 in total ( QC-passed reads + QC-failed reads ) 31894 + 0 secondary 0 + 0 supplementary 0 + 0 duplicates 136447 + 0 mapped ( 86 .92% : N/A ) 0 + 0 paired in sequencing 0 + 0 read1 0 + 0 read2 0 + 0 properly paired ( N/A : N/A ) 0 + 0 with itself and mate mapped 0 + 0 singletons ( N/A : N/A ) 0 + 0 with mate mapped to a different chr 0 + 0 with mate mapped to a different chr ( mapQ> = 5 ) Read Summarization \u00b6 Sequencing reads often need to be assigned to genomic features of interest after they are mapped to the reference genome. This process is often called read summarization or read quantification. Read summarization is required by a number of downstream analyses such as gene expression analysis and histone modification analysis. The output of read summarization is a count table, in which the number of reads assigned to each feature in each library is recorded. Counting \u00b6 We need to do some counting! Want to generate count data for each gene (actually each exon) - how many reads mapped to each exon in the genome, from each of our samples? Once we have that information, we can start thinking about how to determine which genes were differentially expressed in our study. Subread and FeatureCounts \u00b6 The featureCounts tool from the Subread package can be used to count how many reads aligned to each genome feature (exon). Need to specify the annotation informatyion (.gtf file) You can process all the samples at once: $ cd ~/scripting_workshop/rna_seq $ module load Subread/2.0.0-GCC-9.2.0 $ pwd /home/ [ Your_Username ] /scripting_workshop/rna_seq $ mkdir Counts $ cd Counts $ featureCounts -a ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o ./yeast_counts.txt -T 2 -t exon -g gene_id ../Mapping/*sorted.bam Arguments: -a Name of an annotation file. GTF/GFF format by default -o Name of output file including read counts -T Specify the number of threads/CPUs used for mapping. 1 by default -t Specify feature type in GTF annotation. 'exon' by default. Features used for read counting will be extracted from annotation using the provided value. -g Specify attribute type in GTF annotation. 'gene_id' by default. Meta-features used for read counting will be extracted from annotation using the provided value. Group Exercise \u00b6 Now, let's work together in our groups to create an RNA-seq mapping and count script. at this stage we have mastered the art of writing scripts, instead of running them on the commandline, let us now run them on HPC.","title":"RNA-seq Mapping And Count Data Workflow"},{"location":"3_RNAseq/#rna-seq-mapping-and-count-data-workflow","text":"This material is extracted from the RNA-seq workshop Lesson Aim To develop a pipeline that does mapping and count the number of reads that mapped then overall put all these steps into a script. Objectives and overall workflow Understand and perform the steps involved in RNA-seq mapping and read count. Use command line tools to run the pipeline.","title":"RNA-seq Mapping And Count Data Workflow"},{"location":"3_RNAseq/#assumptions","text":"You have already performed trimming and filtering of your reads and saved in a directory called trimmed_reads. You have a reference genome saved in a directory called ref_genome. In this workshop, we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are: $ cd ~/scripting_workshop $ pwd /home/ [ your_username ] /scripting_workshop # good I am ready to work Checking to make sure we have the directory and files for the workshop. $ ls rna_seq variant_calling scheduler hint : If you do not have the workshop directory, you can copy it using the command: cp -r /nesi/project/nesi02659/scripting_workshop/ ~ $ cd rna_seq $ ls ref_genome trimmed_reads","title":"Assumptions"},{"location":"3_RNAseq/#alignment-to-a-reference-genome","text":"RNA-seq generate gene expression information by quantifying the number of transcripts (per gene) in a sample. This is acompished by counting the number of transcripts that have been sequenced - the more active a gene is, the more transcripts will be in a sample, and the more reads will be generated from that transcript. For RNA-seq, we need to align or map each read back to the genome, to see which gene produced it. - Highly expressed genes will generate lots of transcripts, so there will be lots of reads that map back to the position of that transcript in the genome. - The per-gene data we work with in an RNA-seq experiment are counts: the number of reads from each sample that originated from that gene.","title":"Alignment to a reference genome"},{"location":"3_RNAseq/#preparation-of-the-genome","text":"To be able to map (align) sequencing reads on the genome, the genome needs to be indexed first. In this workshop we will use HISAT2 . $ cd ~/scripting_workshop/rna_seq/ref_genome #to list what is in your directory: $ ls ~/scripting_workshop/rna_seq/ref_genome Saccharomyces_cerevisiae.R64-1-1.99.gtf Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa $ module load HISAT2/2.2.0-gimkl-2020a # index file: $ hisat2-build -p 4 -f Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa Saccharomyces_cerevisiae.R64-1-1.dna.toplevel #list what is in the directory: $ ls ~/scripting_workshop/rna_seq/ref_genome Saccharomyces_cerevisiae.R64-1-1.99.gtf Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.4.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.8.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.1.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.5.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.2.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.6.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.3.ht2 Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.7.ht2 Arguments: * -p number of threads * -f fasta file How many files were created during the indexing process?","title":"Preparation of the genome"},{"location":"3_RNAseq/#alignment-on-the-genome","text":"Now that the genome is prepared. Sequencing reads can be aligned. Information required: Where the sequence information is stored (e.g. fastq files ...) ? What kind of sequencing: Single End or Paired end ? Where are stored the indexes and the genome? Where will the mapping files be stored? Now, lets move one folder up (into the rna_seq folder): $ cd .. $ ls ref_genome trimmed_reads Let's map one of our sample to the reference genome $ pwd /home/ [ Your_Username ] /scripting_workshop/rna_seq/ $ mkdir Mapping $ ls ref_genome Mapping trimmed_reads let's use a for loop to process our samples: $ cd trimmed_reads $ ls SRR014335-chr1.fastq SRR014336-chr1.fastq SRR014337-chr1.fastq SRR014339-chr1.fastq SRR014340-chr1.fastq SRR014341-chr1.fastq $ for filename in * > do > base = $( basename ${ filename } .fastq ) > hisat2 -p 4 -x ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S ../Mapping/ ${ base } .sam --summary-file ../Mapping/ ${ base } _summary.txt > done Arguments: * -x The basename of the index for the reference genome. * -U Comma-separated list of files containing unpaired reads to be aligned * -S File to write SAM alignments to. By default, alignments are written to the \u201cstandard out\u201d or \u201cstdout\u201d filehandle Now we can explore our SAM files. $ cd ../Mapping $ ls SRR014335-chr1.sam SRR014336-chr1_summary.txt SRR014339-chr1.sam SRR014340-chr1_summary.txt SRR014335-chr1_summary.txt SRR014337-chr1.sam SRR014339-chr1_summary.txt SRR014341-chr1.sam SRR014336-chr1.sam SRR014337-chr1_summary.txt SRR014340-chr1.sam SRR014341-chr1_summary.txt","title":"Alignment on the genome"},{"location":"3_RNAseq/#converting-sam-files-to-bam-files","text":"The SAM file, is a tab-delimited text file that contains information for each individual read and its alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification. The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file.","title":"Converting SAM files to BAM files"},{"location":"3_RNAseq/#a-quick-look-into-the-sam-file","text":"$ less SRR014335-chr1.sam The file begins with a header, which is optional. The header is used to describe the source of data, reference sequence, method of alignment, etc., this will change depending on the aligner being used. Following the header is the alignment section. Each line that follows corresponds to alignment information for a single read. Each alignment line has 11 mandatory fields for essential mapping information and a variable number of other fields for aligner specific information. An example entry from a SAM file is displayed below with the different fields highlighted. We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format ( -S ) and to output BAM format ( -b ): $ module load SAMtools/1.10-GCC-9.2.0 $ for filename in *.sam > do > base = $( basename ${ filename } .sam ) > samtools view -S -b ${ filename } -o ${ base } .bam > done $ ls SRR014335-chr1.bam SRR014336-chr1.bam SRR014337-chr1.bam SRR014339-chr1.bam SRR014340-chr1.bam SRR014341-chr1.bam SRR014335-chr1.sam SRR014336-chr1.sam SRR014337-chr1.sam SRR014339-chr1.sam SRR014340-chr1.sam SRR014341-chr1.sam Next we sort the BAM file using the sort command from samtools. -o tells the command where to write the output. Note SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input.** $ for filename in *.bam > do > base = $( basename ${ filename } .bam ) > samtools sort -o ${ base } _sorted.bam ${ filename } > done We can use samtools to learn more about the bam file as well.","title":"A quick look into the sam file"},{"location":"3_RNAseq/#some-stats-on-your-mapping","text":"$ samtools flagstat SRR014335-chr1_sorted.bam 156984 + 0 in total ( QC-passed reads + QC-failed reads ) 31894 + 0 secondary 0 + 0 supplementary 0 + 0 duplicates 136447 + 0 mapped ( 86 .92% : N/A ) 0 + 0 paired in sequencing 0 + 0 read1 0 + 0 read2 0 + 0 properly paired ( N/A : N/A ) 0 + 0 with itself and mate mapped 0 + 0 singletons ( N/A : N/A ) 0 + 0 with mate mapped to a different chr 0 + 0 with mate mapped to a different chr ( mapQ> = 5 )","title":"Some stats on your mapping:"},{"location":"3_RNAseq/#read-summarization","text":"Sequencing reads often need to be assigned to genomic features of interest after they are mapped to the reference genome. This process is often called read summarization or read quantification. Read summarization is required by a number of downstream analyses such as gene expression analysis and histone modification analysis. The output of read summarization is a count table, in which the number of reads assigned to each feature in each library is recorded.","title":"Read Summarization"},{"location":"3_RNAseq/#counting","text":"We need to do some counting! Want to generate count data for each gene (actually each exon) - how many reads mapped to each exon in the genome, from each of our samples? Once we have that information, we can start thinking about how to determine which genes were differentially expressed in our study.","title":"Counting"},{"location":"3_RNAseq/#subread-and-featurecounts","text":"The featureCounts tool from the Subread package can be used to count how many reads aligned to each genome feature (exon). Need to specify the annotation informatyion (.gtf file) You can process all the samples at once: $ cd ~/scripting_workshop/rna_seq $ module load Subread/2.0.0-GCC-9.2.0 $ pwd /home/ [ Your_Username ] /scripting_workshop/rna_seq $ mkdir Counts $ cd Counts $ featureCounts -a ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o ./yeast_counts.txt -T 2 -t exon -g gene_id ../Mapping/*sorted.bam Arguments: -a Name of an annotation file. GTF/GFF format by default -o Name of output file including read counts -T Specify the number of threads/CPUs used for mapping. 1 by default -t Specify feature type in GTF annotation. 'exon' by default. Features used for read counting will be extracted from annotation using the provided value. -g Specify attribute type in GTF annotation. 'gene_id' by default. Meta-features used for read counting will be extracted from annotation using the provided value.","title":"Subread and FeatureCounts"},{"location":"3_RNAseq/#group-exercise","text":"Now, let's work together in our groups to create an RNA-seq mapping and count script. at this stage we have mastered the art of writing scripts, instead of running them on the commandline, let us now run them on HPC.","title":"Group Exercise"},{"location":"4_IntroductiontoHPC/","text":"Introduction to HPC \u00b6 Defining high-performance computing \u00b6 The simplest way of defining high-performance computing is by saying that it is the using of high-performance computers (HPC). However, this leads to our next question what is a HPC . A high-performance computer is a network of computers in a cluster that typically share a common purpose and are used to accomplish tasks that might otherwise be too big for any one computer. While modern computers can do a lot (and a lot more than their equivalents 10-20 years ago), there are limits to what they can do and the speed at which they are able to do this. One way to overcome these limits is to pool computers together to create a cluster of computers. These pooled resources can then be used to run software that requires more total memory, or need more processors to complete in a reasonable time. One way to do this is to take a group of computers and link them together via a network switch. Consider a case where you have five 4-core computers. By connecting them together, you could run jobs on 20 cores, which could result in your software running faster. HPC architectures \u00b6 Most HPC systems follow the ideas described above of taking many computers and linking them via network switches. What distinguishes a high-performance computer from the computer clusters described above is: The number of computers/nodes The strength of each individual computer/node The network interconnect \u2013 this dictates the communication speed between nodes. The faster this speed is, the more a group of individual nodes will act like a unit. NeSI Mahuika Cluster architecture \u00b6 NeSI Mahuika cluster (CRAY HPE CS400) system consists of a number of different node types. The ones visible to researchers are: Login nodes Compute nodes In reality Back to homepage","title":"Introduction to HPC"},{"location":"4_IntroductiontoHPC/#introduction-to-hpc","text":"","title":"Introduction to HPC"},{"location":"4_IntroductiontoHPC/#defining-high-performance-computing","text":"The simplest way of defining high-performance computing is by saying that it is the using of high-performance computers (HPC). However, this leads to our next question what is a HPC . A high-performance computer is a network of computers in a cluster that typically share a common purpose and are used to accomplish tasks that might otherwise be too big for any one computer. While modern computers can do a lot (and a lot more than their equivalents 10-20 years ago), there are limits to what they can do and the speed at which they are able to do this. One way to overcome these limits is to pool computers together to create a cluster of computers. These pooled resources can then be used to run software that requires more total memory, or need more processors to complete in a reasonable time. One way to do this is to take a group of computers and link them together via a network switch. Consider a case where you have five 4-core computers. By connecting them together, you could run jobs on 20 cores, which could result in your software running faster.","title":"Defining high-performance computing"},{"location":"4_IntroductiontoHPC/#hpc-architectures","text":"Most HPC systems follow the ideas described above of taking many computers and linking them via network switches. What distinguishes a high-performance computer from the computer clusters described above is: The number of computers/nodes The strength of each individual computer/node The network interconnect \u2013 this dictates the communication speed between nodes. The faster this speed is, the more a group of individual nodes will act like a unit.","title":"HPC architectures"},{"location":"4_IntroductiontoHPC/#nesi-mahuika-cluster-architecture","text":"NeSI Mahuika cluster (CRAY HPE CS400) system consists of a number of different node types. The ones visible to researchers are: Login nodes Compute nodes In reality Back to homepage","title":"NeSI Mahuika Cluster architecture"},{"location":"5_working_with_job_scheduler/","text":"Working with job scheduler \u00b6 Introduction to slurm scheduler and directives \u00b6 An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when. In brief, scheduler is a Mechanism to control access by many users to shared computing resources Queuing / scheduling system for users\u2019 jobs Manages the reservation of resources and job execution on these resources Allows users to \u201cfire and forget\u201d large, long calculations or many jobs (\u201cproduction runs\u201d) Why do we need a scheduler ? To ensure the machine is utilised as fully as possible To ensure all users get a fair chance to use compute resources (demand usually exceeds supply) To track usage - for accounting and budget control To mediate access to other resources e.g. software licences Commonly used schedulers Slurm PBS , Torque Grid Engine All NeSI clusters use Slurm (Simple Linux Utility for Resource Management) scheduler (or job submission system) to manage resources and how they are made available to users. The main commands you will use with Slurm on NeSI Mahuika cluster are: A quick note on sinfo (Query the current state of nodes) which is not a command a researcher will use regularly but helps HPC admins and support staff with monitoring. Life cycle of a slurm job \u00b6 Command Function sbatch Submit non-interactive (batch) jobs to the scheduler squeue List jobs in the queue scancel Cancel a job sacct Display accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database srun Slurm directive for parallel computing sinfo Query the current state of nodes salloc Submit interactive jobs to the scheduler Exercise 5.1 summary of current states of compute nodes known to the scheduler sinfo similar to above but expanded sinfo --format = \"%16P %.8m %.5a %10T %.5D %80N\" will print a long output as it is one row per compute node in the cluster sinfo -N -l Explore the capacity of a compute node sinfo -n wch001 -o \"%n %c %m\" Anatomy of a slurm script and submitting first slurm job \ud83e\uddd0 \u00b6 As with most other scheduler systems, job submission scripts in Slurm consist of a header section with the shell specification and options to the submission command ( sbatch in this case) followed by the body of the script that actually runs the commands you want. In the header section, options to sbatch should be prepended with #SBATCH . Commented lines are ignored by the bash interpreter, but they are not ignored by slurm. The #SBATCH parameters are read by slurm when we submit the job. When the job starts, the bash interpreter will ignore all lines starting with # . This is very similar to the shebang mentioned earlier, when you run your script, the system looks at the #! , then uses the program at the subsequent path to interpret the script, in our case /bin/bash (the program bash found in the /bin directory header use description --job-name #SBATCH --job-name=MyJob The name that will appear when using squeue or sacct. --account #SBATCH --account=nesi12345 The account your core hours will be 'charged' to. --time #SBATCH --time=DD-HH:MM:SS Job max walltime. --mem #SBATCH --mem=512MB Memory required per node. --cpus-per-task #SBATCH --cpus-per-task=10 Will request 10 logical CPUs per task. --output #SBATCH --output=%j_output.out Path and name of standard output file. %j will be replaced by the job ID. --mail-user #SBATCH --mail-user=me23@gmail.com address to send mail notifications. --mail-type #SBATCH --mail-type=ALL Will send a mail notification at BEGIN END FAIL. #SBATCH --mail-type=TIME_LIMIT_80 Will send message at 80% walltime. Exercise 5.2 Let's put these directives together and compile our first slurm script First create a new working directory and end the directory cd ~/scripting_workshop/scheduler confirm the path is correct pwd create a new directory for this section and change the directory to it - Check for the follow up not && mkdir ex_5.2 && cd ex_5.2 use a text editor of choice to create a file named firstslurm.sl - we will use nano here nano firstslurm.sl Content of firstslurm.sl should be as below. Please discuss as you make progress #!/bin/bash #SBATCH --job-name myfirstslurmjob #SBATCH --account nesi02659 #SBATCH --time 00:01:00 #SBATCH --cpus-per-task 1 #SBATCH --mem 512 #SBATCH --output slurmjob.%j.out sleep 200 echo \"I am a slurm job and I slept for 200 seconds\" echo \" $SLURM_JOB_ID END\" Save and Exit Submit the script with sbatch command sbatch firstslurm.sl * Execute squeue --me and sacct . Discuss the outputs .i.e. squeue --me sacct The meaning of && and & are intrinsically different. What is && in Bash? In Bash\u2014and many other programming languages\u2014 && means \u201cAND\u201d. And in command execution context like this, it means items to the left as well as right of && should be run in sequence in this case. What is & in Bash? And a single & means that the preceding commands\u2014to the immediate left of the &\u2014should simply be run in the background. STDOUT/STDERR from jobs \u00b6 STDOUT - your process writes conventional output to this file handle STDERR - your process writes diagnostic output to this file handle. STDOUT and STDERR from jobs are, by default, written to a file called slurm-JOBID.out and slurm-JOBID.err in the working directory for the job (unless the job script changes this, this will be the directory where you submitted the job). So for a job with ID 12345 STDOUT and STDERR will be slurm-12345.out and slurm-12345.err . When things go wrong, first step of debugging (STORY TIME !) starts with a referral to these files. Assessing resource utilisation (cpu, memory, time) \u00b6 Understanding the resources you have available and how to use them most efficiently is a vital skill in high performance computing. The three resources that every single job submitted on the platform needs to request are: CPUs (i.e. logical CPU cores), and Memory (RAM), and Time. What happens if I ask for the wrong resources? Resource Asking for too much Not asking for enough Number of CPUs Job may wait in the queue for longer Job will run more slowly than expected, and so may run out time Drop in fairshare score which determines job priority Memory (above) Job will fail, probably with OUT OF MEMORY error, segmentation fault or bus error Wall time (above) Job will run out of time and get killed Exercise 5.3 Let's submit another slurm job and review its resource utilisation Change the working directory to Exercise_5.3 cd ~/scripting_workshop/scheduler/ex_5.3 Run ls command and you should see two files (one .R and one sl) and one directory named slurmout ls -F bowtie-test.sl* input_data/ slurmout/ Review the slurm script bowtie-test.sl with nano and edit the corresponding sections (hint :email) sbatch bowtie-test.sl use squeue --me and sacct again to evaluate the job status Once the job ran into completion, use nn_seff JOBID command to print the resource utilisation statistics (Replace JOBID with the corresponding number) $ nn_seff 25222190 Job ID: 25222190 Cluster: mahuika User/Group: me1234/me1234 State: COMPLETED ( exit code 0 ) Cores: 1 Tasks: 1 Nodes: 1 Job Wall-time: 18 .33% 00 :00:33 of 00 :03:00 time limit CPU Efficiency: 93 .94% 00 :00:31 of 00 :00:33 core-walltime Mem Efficiency: 1 .33% 13 .62 MB of 1 .00 GB Now review the content of .err and .out files in /slurmout directory Feeling adventurous ? - Refer to Supplementary material on slurm profiling Compiling slurm scripts for Variant Calling and RNA-seq episodes \u00b6 Exercise 5.4 \ud83d\ude2c Purpose of this exercise is to compile a slurm submission script based on the script we wrote in episode 2 - Automating variant calling workflow recommend creating a new directory for the exercise .i.e ex_5.4 Name of the file is variant_calling.sl (note that we have change the extension from .sh to .sl ) In terms of slurm variables name of the job is variant_calling_workflow number of CPUS is 2 timelimit 15 minutes amount of memory in GB 4G generate .err files and .out where both should be re-directed to the directory slurmout an email notification at the end of the job We don't want to replicate input data in multiple places .i.e. be conservative in-terms how you use research storage Therefore, use the same reference genome file (assign the filename to variable genome and the trimmed read files (assign the path of these files to variable trimmed ) used in the first episode genome = ~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta trimmed = ~/scripting_workshop/variant_calling/trimmed_reads Exercise 5.5 \ud83d\ude2c Now it's your turn to compile a slurm submission script for the RNA-seq workflow. \ud83d\ude0a Back to homepage","title":"Working with job scheduler"},{"location":"5_working_with_job_scheduler/#working-with-job-scheduler","text":"","title":"Working with job scheduler"},{"location":"5_working_with_job_scheduler/#introduction-to-slurm-scheduler-and-directives","text":"An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when. In brief, scheduler is a Mechanism to control access by many users to shared computing resources Queuing / scheduling system for users\u2019 jobs Manages the reservation of resources and job execution on these resources Allows users to \u201cfire and forget\u201d large, long calculations or many jobs (\u201cproduction runs\u201d) Why do we need a scheduler ? To ensure the machine is utilised as fully as possible To ensure all users get a fair chance to use compute resources (demand usually exceeds supply) To track usage - for accounting and budget control To mediate access to other resources e.g. software licences Commonly used schedulers Slurm PBS , Torque Grid Engine All NeSI clusters use Slurm (Simple Linux Utility for Resource Management) scheduler (or job submission system) to manage resources and how they are made available to users. The main commands you will use with Slurm on NeSI Mahuika cluster are: A quick note on sinfo (Query the current state of nodes) which is not a command a researcher will use regularly but helps HPC admins and support staff with monitoring.","title":"Introduction to slurm scheduler and directives"},{"location":"5_working_with_job_scheduler/#life-cycle-of-a-slurm-job","text":"Command Function sbatch Submit non-interactive (batch) jobs to the scheduler squeue List jobs in the queue scancel Cancel a job sacct Display accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database srun Slurm directive for parallel computing sinfo Query the current state of nodes salloc Submit interactive jobs to the scheduler Exercise 5.1 summary of current states of compute nodes known to the scheduler sinfo similar to above but expanded sinfo --format = \"%16P %.8m %.5a %10T %.5D %80N\" will print a long output as it is one row per compute node in the cluster sinfo -N -l Explore the capacity of a compute node sinfo -n wch001 -o \"%n %c %m\"","title":"Life cycle of a slurm job"},{"location":"5_working_with_job_scheduler/#anatomy-of-a-slurm-script-and-submitting-first-slurm-job","text":"As with most other scheduler systems, job submission scripts in Slurm consist of a header section with the shell specification and options to the submission command ( sbatch in this case) followed by the body of the script that actually runs the commands you want. In the header section, options to sbatch should be prepended with #SBATCH . Commented lines are ignored by the bash interpreter, but they are not ignored by slurm. The #SBATCH parameters are read by slurm when we submit the job. When the job starts, the bash interpreter will ignore all lines starting with # . This is very similar to the shebang mentioned earlier, when you run your script, the system looks at the #! , then uses the program at the subsequent path to interpret the script, in our case /bin/bash (the program bash found in the /bin directory header use description --job-name #SBATCH --job-name=MyJob The name that will appear when using squeue or sacct. --account #SBATCH --account=nesi12345 The account your core hours will be 'charged' to. --time #SBATCH --time=DD-HH:MM:SS Job max walltime. --mem #SBATCH --mem=512MB Memory required per node. --cpus-per-task #SBATCH --cpus-per-task=10 Will request 10 logical CPUs per task. --output #SBATCH --output=%j_output.out Path and name of standard output file. %j will be replaced by the job ID. --mail-user #SBATCH --mail-user=me23@gmail.com address to send mail notifications. --mail-type #SBATCH --mail-type=ALL Will send a mail notification at BEGIN END FAIL. #SBATCH --mail-type=TIME_LIMIT_80 Will send message at 80% walltime. Exercise 5.2 Let's put these directives together and compile our first slurm script First create a new working directory and end the directory cd ~/scripting_workshop/scheduler confirm the path is correct pwd create a new directory for this section and change the directory to it - Check for the follow up not && mkdir ex_5.2 && cd ex_5.2 use a text editor of choice to create a file named firstslurm.sl - we will use nano here nano firstslurm.sl Content of firstslurm.sl should be as below. Please discuss as you make progress #!/bin/bash #SBATCH --job-name myfirstslurmjob #SBATCH --account nesi02659 #SBATCH --time 00:01:00 #SBATCH --cpus-per-task 1 #SBATCH --mem 512 #SBATCH --output slurmjob.%j.out sleep 200 echo \"I am a slurm job and I slept for 200 seconds\" echo \" $SLURM_JOB_ID END\" Save and Exit Submit the script with sbatch command sbatch firstslurm.sl * Execute squeue --me and sacct . Discuss the outputs .i.e. squeue --me sacct The meaning of && and & are intrinsically different. What is && in Bash? In Bash\u2014and many other programming languages\u2014 && means \u201cAND\u201d. And in command execution context like this, it means items to the left as well as right of && should be run in sequence in this case. What is & in Bash? And a single & means that the preceding commands\u2014to the immediate left of the &\u2014should simply be run in the background.","title":"Anatomy of a slurm script and submitting first slurm job \ud83e\uddd0"},{"location":"5_working_with_job_scheduler/#stdoutstderr-from-jobs","text":"STDOUT - your process writes conventional output to this file handle STDERR - your process writes diagnostic output to this file handle. STDOUT and STDERR from jobs are, by default, written to a file called slurm-JOBID.out and slurm-JOBID.err in the working directory for the job (unless the job script changes this, this will be the directory where you submitted the job). So for a job with ID 12345 STDOUT and STDERR will be slurm-12345.out and slurm-12345.err . When things go wrong, first step of debugging (STORY TIME !) starts with a referral to these files.","title":"STDOUT/STDERR from jobs"},{"location":"5_working_with_job_scheduler/#assessing-resource-utilisation-cpu-memory-time","text":"Understanding the resources you have available and how to use them most efficiently is a vital skill in high performance computing. The three resources that every single job submitted on the platform needs to request are: CPUs (i.e. logical CPU cores), and Memory (RAM), and Time. What happens if I ask for the wrong resources? Resource Asking for too much Not asking for enough Number of CPUs Job may wait in the queue for longer Job will run more slowly than expected, and so may run out time Drop in fairshare score which determines job priority Memory (above) Job will fail, probably with OUT OF MEMORY error, segmentation fault or bus error Wall time (above) Job will run out of time and get killed Exercise 5.3 Let's submit another slurm job and review its resource utilisation Change the working directory to Exercise_5.3 cd ~/scripting_workshop/scheduler/ex_5.3 Run ls command and you should see two files (one .R and one sl) and one directory named slurmout ls -F bowtie-test.sl* input_data/ slurmout/ Review the slurm script bowtie-test.sl with nano and edit the corresponding sections (hint :email) sbatch bowtie-test.sl use squeue --me and sacct again to evaluate the job status Once the job ran into completion, use nn_seff JOBID command to print the resource utilisation statistics (Replace JOBID with the corresponding number) $ nn_seff 25222190 Job ID: 25222190 Cluster: mahuika User/Group: me1234/me1234 State: COMPLETED ( exit code 0 ) Cores: 1 Tasks: 1 Nodes: 1 Job Wall-time: 18 .33% 00 :00:33 of 00 :03:00 time limit CPU Efficiency: 93 .94% 00 :00:31 of 00 :00:33 core-walltime Mem Efficiency: 1 .33% 13 .62 MB of 1 .00 GB Now review the content of .err and .out files in /slurmout directory Feeling adventurous ? - Refer to Supplementary material on slurm profiling","title":"Assessing resource utilisation (cpu, memory, time)"},{"location":"5_working_with_job_scheduler/#compiling-slurm-scripts-for-variant-calling-and-rna-seq-episodes","text":"Exercise 5.4 \ud83d\ude2c Purpose of this exercise is to compile a slurm submission script based on the script we wrote in episode 2 - Automating variant calling workflow recommend creating a new directory for the exercise .i.e ex_5.4 Name of the file is variant_calling.sl (note that we have change the extension from .sh to .sl ) In terms of slurm variables name of the job is variant_calling_workflow number of CPUS is 2 timelimit 15 minutes amount of memory in GB 4G generate .err files and .out where both should be re-directed to the directory slurmout an email notification at the end of the job We don't want to replicate input data in multiple places .i.e. be conservative in-terms how you use research storage Therefore, use the same reference genome file (assign the filename to variable genome and the trimmed read files (assign the path of these files to variable trimmed ) used in the first episode genome = ~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta trimmed = ~/scripting_workshop/variant_calling/trimmed_reads Exercise 5.5 \ud83d\ude2c Now it's your turn to compile a slurm submission script for the RNA-seq workflow. \ud83d\ude0a Back to homepage","title":"Compiling slurm scripts for Variant Calling and RNA-seq episodes"},{"location":"6_supplementary_1/","text":"S1 : Accessing software via modules \u00b6 On a high-performance computing system, it is quite rare that the software we want to use is available when we log in. It is installed, but we will need to \u201cload\u201d it before it can run. Before we start using individual software packages, however, we should understand the reasoning behind this approach. The three biggest factors are: software incompatibilities versioning dependencies One of the workarounds for this issue is Environment modules. A module is a self-contained description of a software package \u2014 it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. There are a number of different environment module implementations commonly used on HPC systems and the one used in NeSI Mahuika cluster is Lmod where the module command is used to interact with environment modules. Commonly used module sub-commands View available modules module avail View all modules which match the keyword in their name module avail KEYWORD View all modules which match the keyword in their name or description module spider KEYWORD Load a specific program Note: All modules on NeSI have version and toolchain/environment suffixes. If none is specified, the default version for the tool is loaded. The default version can be seen with the module avail command. module load MY_APPLICATION Swap a currently loaded module for a different one module switch CURRENT_MODULE DESIRED_MODULE Unload all current modules module purge Danger Please do not use $ module --force purge Back to homepage","title":"S1 : Accessing software via modules"},{"location":"6_supplementary_1/#s1-accessing-software-via-modules","text":"On a high-performance computing system, it is quite rare that the software we want to use is available when we log in. It is installed, but we will need to \u201cload\u201d it before it can run. Before we start using individual software packages, however, we should understand the reasoning behind this approach. The three biggest factors are: software incompatibilities versioning dependencies One of the workarounds for this issue is Environment modules. A module is a self-contained description of a software package \u2014 it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. There are a number of different environment module implementations commonly used on HPC systems and the one used in NeSI Mahuika cluster is Lmod where the module command is used to interact with environment modules. Commonly used module sub-commands View available modules module avail View all modules which match the keyword in their name module avail KEYWORD View all modules which match the keyword in their name or description module spider KEYWORD Load a specific program Note: All modules on NeSI have version and toolchain/environment suffixes. If none is specified, the default version for the tool is loaded. The default version can be seen with the module avail command. module load MY_APPLICATION Swap a currently loaded module for a different one module switch CURRENT_MODULE DESIRED_MODULE Unload all current modules module purge Danger Please do not use $ module --force purge Back to homepage","title":"S1 : Accessing software via modules"},{"location":"7_supplementary_2/","text":"S2 : slurm profiling \u00b6 Although nn_seff command is a quick and easy way to determine the resource utilisation, it relies on peak values (data gets recorded every 30 seconds) which doesn't allows us to examine resource usage over the run-time of the job. There are number of in-built/external tools to achieve the latter which will require some effort to understand its deployment, tracing and interpretation. Therefore, we will use slurm native profiling to evaluate resource usage over run-time. This is a simple and elegant solution. Exercise S.2.1 Download and decompress the content wget -c Exercise_S21.tar.gz https://github.com/DininduSenanayake/nesi_introductory_custom/releases/download/v1.0/Exercise_S21.tar.gz -O - | tar -xz cd Exercise_S21 Run ls command and you should see three files (one .R,sl and one .py - We will discuss the purpose of this .py file after submitting the job) and one directory named slurmout ls -F example1_arraysum.R example1_arraysum.sl profile_plot_Jul2020.py slurmout/ Review the slurm script with cat Or another text editor and submit with sbatch sbatch example1_arraysum.sl Do take a note of the JOBID as we are going to need it for next step. Otherwise, we use squeue --me OR sacct command as before to monitor the status Also, you can watch the status of this job via $ watch -n 1 -d \"squeue -j JOBID\" . watch command execute a program periodically, showing output fullscreen. Exiting the watch screen by done by pressing Ctrl+x Let's create slurm profile graphs collate the data into an HDF5 file using the command. Replace JOBID with the corresponding number sh5util -j JOBID sh5util: Merging node-step files into ./job_JOBID.h5 execute the script on .h5 file. We will need one of the Python 3 modules to do this. Ignore the deprecating warning. module purge module load Python/3.8.2-gimkl-2020a Replace JOBID with the corresponding number python profile_plot_Jul2020.py job_JOBID.h5 This should generate a .png file where the filename is in the format of job_23258404_profile.png Back to homepage","title":"S2 : slurm profiling"},{"location":"7_supplementary_2/#s2-slurm-profiling","text":"Although nn_seff command is a quick and easy way to determine the resource utilisation, it relies on peak values (data gets recorded every 30 seconds) which doesn't allows us to examine resource usage over the run-time of the job. There are number of in-built/external tools to achieve the latter which will require some effort to understand its deployment, tracing and interpretation. Therefore, we will use slurm native profiling to evaluate resource usage over run-time. This is a simple and elegant solution. Exercise S.2.1 Download and decompress the content wget -c Exercise_S21.tar.gz https://github.com/DininduSenanayake/nesi_introductory_custom/releases/download/v1.0/Exercise_S21.tar.gz -O - | tar -xz cd Exercise_S21 Run ls command and you should see three files (one .R,sl and one .py - We will discuss the purpose of this .py file after submitting the job) and one directory named slurmout ls -F example1_arraysum.R example1_arraysum.sl profile_plot_Jul2020.py slurmout/ Review the slurm script with cat Or another text editor and submit with sbatch sbatch example1_arraysum.sl Do take a note of the JOBID as we are going to need it for next step. Otherwise, we use squeue --me OR sacct command as before to monitor the status Also, you can watch the status of this job via $ watch -n 1 -d \"squeue -j JOBID\" . watch command execute a program periodically, showing output fullscreen. Exiting the watch screen by done by pressing Ctrl+x Let's create slurm profile graphs collate the data into an HDF5 file using the command. Replace JOBID with the corresponding number sh5util -j JOBID sh5util: Merging node-step files into ./job_JOBID.h5 execute the script on .h5 file. We will need one of the Python 3 modules to do this. Ignore the deprecating warning. module purge module load Python/3.8.2-gimkl-2020a Replace JOBID with the corresponding number python profile_plot_Jul2020.py job_JOBID.h5 This should generate a .png file where the filename is in the format of job_23258404_profile.png Back to homepage","title":"S2 : slurm profiling"},{"location":"8_supplementary_3/","text":"S3 : Solutions \u00b6 \u00ab7. Supplementary 2 Exercise 5.4 \ud83d\ude2c #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name variant_calling_workflow #SBATCH --cpus-per-task 2 #SBATCH --time 00:15:00 #SBATCH --mem 4G #SBATCH --output slurmout/variant_calling-%j.out #SBATCH --error slurmout/variant_calling-%j.err #SBATCH --mail-type END #SBATCH --mail-user myemail@email.co.nz # Load all the required modules module purge module load BWA/0.7.17-GCC-9.2.0 module load SAMtools/1.13-GCC-9.2.0 module load BCFtools/1.13-GCC-9.2.0 echo \" $PWD \" # create the results directories mkdir -p results/sam results/bam results/bcf results/vcf # indexing the genome genome = ~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta trimmed = ~/scripting_workshop/variant_calling/trimmed_reads bwa index $genome # create a loop that map reads to the genome, sort the bam files and call variants for fq1 in ${ trimmed } /*_1.trim.sub.fastq do echo \"working with file $fq1 \" base = $( basename $fq1 _1.trim.sub.fastq ) echo \"base name is $base \" # setting the variables fq1 = ${ trimmed } / ${ base } _1.trim.sub.fastq fq2 = ${ trimmed } / ${ base } _2.trim.sub.fastq sam = results/sam/ ${ base } .aligned.sam bam = results/bam/ ${ base } .aligned.bam sorted_bam = results/bam/ ${ base } .aligned.sorted.bam raw_bcf = results/bcf/ ${ base } _raw.bcf variants = results/vcf/ ${ base } _variants.vcf final_variants = results/vcf/ ${ base } _final_variants.vcf # running the analysis steps bwa mem $genome $fq1 $fq2 > $sam samtools view -S -b $sam > $bam samtools sort -o $sorted_bam $bam samtools index $sorted_bam bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam bcftools call --ploidy 1 -m -v -o $variants $raw_bcf vcfutils.pl varFilter $variants > $final_variants done echo \"DONE\" Exercise 5.5 \ud83d\ude2c #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name rna-seq_workflow #SBATCH --cpus-per-task 2 #SBATCH --time 00:15:00 #SBATCH --mem 4G #SBATCH --output rna-seq_workflow-%j.out #SBATCH --error rna-seq_workflow-%j.err #SBATCH --mail-type END #SBATCH --mail-user myemail@email.org.nz echo \" $PWD \" mkdir -p ~/scripting_workshop/scheduler/ex_5.5/ { Mapping,Counts } && cd ~/scripting_workshop/scheduler/ex_5.5/ cp -r /nesi/project/nesi02659/scripting_workshop/rna_seq/* ./ module purge module load HISAT2/2.2.0-gimkl-2020a module load SAMtools/1.10-GCC-9.2.0 module load Subread/2.0.0-GCC-9.2.0 echo $PWD #index file hisat2-build -p 4 -f $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel #Mapping Samples to the reference genome for filename in $PWD /trimmed_reads/* do base = $( basename ${ filename } .fastq ) hisat2 -p 4 -x $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S $PWD /Mapping/ ${ base } .sam --summary-file $PWD /Mapping/ ${ base } _summary.txt done #Convert SAMfiles to BAM for filename in $PWD /Mapping/*.sam do base = $( basename ${ filename } .sam ) samtools view -S -b ${ filename } -o $PWD /Mapping/ ${ base } .bam done #Sort BAM files for filename in $PWD /Mapping/*.bam do base = $( basename ${ filename } .bam ) samtools sort -o $PWD /Mapping/ ${ base } _sorted.bam ${ filename } done #count how many reads aligned to each genome feature (exon). featureCounts -a $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o $PWD /Counts/yeast_counts.txt -T 2 -t exon -g gene_id $PWD /Mapping/*sorted.bam Back to homepage","title":"S3 : Solutions"},{"location":"8_supplementary_3/#s3-solutions","text":"\u00ab7. Supplementary 2 Exercise 5.4 \ud83d\ude2c #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name variant_calling_workflow #SBATCH --cpus-per-task 2 #SBATCH --time 00:15:00 #SBATCH --mem 4G #SBATCH --output slurmout/variant_calling-%j.out #SBATCH --error slurmout/variant_calling-%j.err #SBATCH --mail-type END #SBATCH --mail-user myemail@email.co.nz # Load all the required modules module purge module load BWA/0.7.17-GCC-9.2.0 module load SAMtools/1.13-GCC-9.2.0 module load BCFtools/1.13-GCC-9.2.0 echo \" $PWD \" # create the results directories mkdir -p results/sam results/bam results/bcf results/vcf # indexing the genome genome = ~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta trimmed = ~/scripting_workshop/variant_calling/trimmed_reads bwa index $genome # create a loop that map reads to the genome, sort the bam files and call variants for fq1 in ${ trimmed } /*_1.trim.sub.fastq do echo \"working with file $fq1 \" base = $( basename $fq1 _1.trim.sub.fastq ) echo \"base name is $base \" # setting the variables fq1 = ${ trimmed } / ${ base } _1.trim.sub.fastq fq2 = ${ trimmed } / ${ base } _2.trim.sub.fastq sam = results/sam/ ${ base } .aligned.sam bam = results/bam/ ${ base } .aligned.bam sorted_bam = results/bam/ ${ base } .aligned.sorted.bam raw_bcf = results/bcf/ ${ base } _raw.bcf variants = results/vcf/ ${ base } _variants.vcf final_variants = results/vcf/ ${ base } _final_variants.vcf # running the analysis steps bwa mem $genome $fq1 $fq2 > $sam samtools view -S -b $sam > $bam samtools sort -o $sorted_bam $bam samtools index $sorted_bam bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam bcftools call --ploidy 1 -m -v -o $variants $raw_bcf vcfutils.pl varFilter $variants > $final_variants done echo \"DONE\" Exercise 5.5 \ud83d\ude2c #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name rna-seq_workflow #SBATCH --cpus-per-task 2 #SBATCH --time 00:15:00 #SBATCH --mem 4G #SBATCH --output rna-seq_workflow-%j.out #SBATCH --error rna-seq_workflow-%j.err #SBATCH --mail-type END #SBATCH --mail-user myemail@email.org.nz echo \" $PWD \" mkdir -p ~/scripting_workshop/scheduler/ex_5.5/ { Mapping,Counts } && cd ~/scripting_workshop/scheduler/ex_5.5/ cp -r /nesi/project/nesi02659/scripting_workshop/rna_seq/* ./ module purge module load HISAT2/2.2.0-gimkl-2020a module load SAMtools/1.10-GCC-9.2.0 module load Subread/2.0.0-GCC-9.2.0 echo $PWD #index file hisat2-build -p 4 -f $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel #Mapping Samples to the reference genome for filename in $PWD /trimmed_reads/* do base = $( basename ${ filename } .fastq ) hisat2 -p 4 -x $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S $PWD /Mapping/ ${ base } .sam --summary-file $PWD /Mapping/ ${ base } _summary.txt done #Convert SAMfiles to BAM for filename in $PWD /Mapping/*.sam do base = $( basename ${ filename } .sam ) samtools view -S -b ${ filename } -o $PWD /Mapping/ ${ base } .bam done #Sort BAM files for filename in $PWD /Mapping/*.bam do base = $( basename ${ filename } .bam ) samtools sort -o $PWD /Mapping/ ${ base } _sorted.bam ${ filename } done #count how many reads aligned to each genome feature (exon). featureCounts -a $PWD /ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o $PWD /Counts/yeast_counts.txt -T 2 -t exon -g gene_id $PWD /Mapping/*sorted.bam Back to homepage","title":"S3 : Solutions"}]}
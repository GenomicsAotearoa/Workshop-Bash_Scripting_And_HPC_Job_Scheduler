{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction-to-bash-scripting-and-hpc-scheduler","title":"Introduction to Bash Scripting and HPC Scheduler","text":"<p>This as an Introductory level workshop on Bash Scripting and HPC Job Scheduler, Slurm.</p> <p>Prerequisites</p> <ul> <li>Familiarity with terminal and basic linux commands</li> <li>Intermediate level knowledge on Molecular Biology and Genetics </li> </ul> <p>Some of the things we won't cover in this workshop</p> <ul> <li>Domain specific concepts in<ul> <li>Genetics, Genomics and DNA Sequencing </li> <li>Variant Calling</li> <li>RNA sequencing and data analysis</li> </ul> </li> </ul> <p>Setup</p> <p>Workshop material is designed to run on NeSI Mahuika cluster via Jupyter. Instructions on how to Set/Reset Authentication factors to access NeSI Services and Jupyter Login instructions can be found here</p> <p>Content</p> Lesson Overview 1. Designing a Variant Calling Workflow 2. Automating a Variant Calling Workflow 3. RNA-seq Mapping And Count Data Workflow 4. Introduction to HPC Introduction to High Performance Computing 5. Working with Job Scheduler Introduction to HPC Job Schedulers, Slurm Scheduler &amp; life cycle of a Slurm job, Assessing resource utilisation and profiling 6. Supplementary #1 7. Supplementary #2 8. Supplementary #3"},{"location":"1_DesigningVariantC/","title":"Variant Calling Workflow","text":"<p>This material is extracted from the Genomics Data Carpentry Lesson</p> <p>Objectives and overall workflow</p> <ul> <li>Understand and perform the steps involved in variant calling.</li> <li>Describe the types of data formats encountered during variant calling.</li> <li>Use command line tools to perform variant calling.</li> </ul> <p> </p> <p>Assumptions</p> <ul> <li>You have already performed trimming and filtering of your reads and saved in a directory called trimmed_reads.</li> <li>You have a reference genome saved in a directory called <code>ref_genome</code>.</li> </ul> <p>In this workshop, we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are:</p> <p>script</p> <p><pre><code>cd ~\n</code></pre> <pre><code>pwd\n</code></pre></p> <p>Checking to make sure we have the directory and files for the workshop.</p> <p>script</p> <pre><code>ls\n</code></pre> <ul> <li>You should see a directory names scripting_workshop</li> </ul> <p>Quick Check</p> <p>If you do not have the workshop directory, you can copy it using the command: <code>cp -r  /nesi/project/nesi02659/scripting_workshop/ ~</code> </p> <p><pre><code>cd scripting_workshop/variant_calling\n</code></pre> <pre><code>$ ls\nref_genome  trimmed_reads </code></pre></p>"},{"location":"1_DesigningVariantC/#alignment-to-a-reference-genome","title":"Alignment to a reference genome","text":"<p>First we need to create directories for the results that will be generated as part of this workflow. We can do this in a single line of code, because mkdir can accept multiple new directory names as input.</p> <pre><code>mkdir -p results/sam results/bam results/bcf results/vcf\n</code></pre>"},{"location":"1_DesigningVariantC/#index-the-reference-genome","title":"Index the reference genome","text":"<p>Our first step is to index the reference genome for use by BWA. Indexing allows the aligner to quickly find potential alignment sites for query sequences in a genome, which saves time during alignment. Indexing the reference only has to be run once. The only reason you would want to create a new index is if you are working with a different reference genome or you are using a different tool for alignment.</p> <p>Since we are working on the NeSI HPC, we need to search and load the package before we start using it. - More on packages will be discussed in the HPC and Slurm section</p> <p>Search <pre><code>module spider bwa\n</code></pre></p> <p>and then load BWA module. </p> <pre><code>module purge\nmodule load BWA/0.7.17-GCC-9.2.0\n</code></pre> <p>indexing the genome <pre><code>bwa index ref_genome/ecoli_rel606.fasta\n</code></pre></p> Output <pre><code>[bwa_index] Pack FASTA... 0.03 sec\n[bwa_index] Construct BWT for the packed sequence...\n[bwa_index] 1.04 seconds elapse.\n[bwa_index] Update BWT... 0.03 sec\n[bwa_index] Pack forward-only FASTA... 0.02 sec\n[bwa_index] Construct SA from BWT and Occ... 0.57 sec\n[main] Version: 0.7.17-r1188\n[main] CMD: bwa index ref_genome/ecoli_rel606.fasta\n[main] Real time: 2.462 sec; CPU: 1.702 sec\n</code></pre>"},{"location":"1_DesigningVariantC/#align-reads-to-reference-genome","title":"Align reads to reference genome","text":"<p>The alignment process consists of choosing an appropriate reference genome to map our reads against and then deciding on an aligner. We will use the BWA-MEM algorithm, which is the latest and is generally recommended for high-quality queries as it is faster and more accurate. We are going to start by aligning the reads from just one of the samples in our dataset (SRR2584866).</p> <p><pre><code>$ bwa mem ref_genome/ecoli_rel606.fasta trimmed_reads/SRR2584866_1.trim.sub.fastq trimmed_reads/SRR2584866_2.trim.sub.fastq &gt; results/sam/SRR2584866.aligned.sam\n\n[M::bwa_idx_load_from_disk] read 0 ALT contigs\n[M::process] read 77446 sequences (10000033 bp)...\n[M::process] read 77296 sequences (10000182 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (48, 36728, 21, 61)\n[M::mem_pestat] analyzing insert size distribution for orientation FF...\n[M::mem_pestat] (25, 50, 75) percentile: (420, 660, 1774)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 4482)\n.....\n</code></pre> <pre><code>$ ls results/sam/\nSRR2584866.aligned.sam </code></pre></p>"},{"location":"1_DesigningVariantC/#sambam-format","title":"SAM/BAM format","text":"<p>The SAM file, is a tab-delimited text file that contains information for each individual read and its alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification.</p> <p>The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file.</p> <p>We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (-S) and to output BAM format (-b):</p> <p>We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (-S) and to output BAM format (-b):</p> <p><pre><code>module load SAMtools/1.13-GCC-9.2.0\n</code></pre> <pre><code>samtools view -S -b results/sam/SRR2584866.aligned.sam &gt; results/bam/SRR2584866.aligned.bam\n</code></pre></p>"},{"location":"1_DesigningVariantC/#sort-bam-file-by-coordinates","title":"Sort BAM file by coordinates","text":"<p>Next we sort the BAM file using the <code>sort</code> command from samtools. -o tells the command where to write the output.</p> <pre><code>samtools sort -o results/bam/SRR2584866.aligned.sorted.bam results/bam/SRR2584866.aligned.bam\n</code></pre> <p>hint: SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input.</p> <p>You can use samtools to learn more about this bam file as well. <pre><code>samtools flagstat results/bam/SRR2584866.aligned.sorted.bam\n</code></pre></p>"},{"location":"1_DesigningVariantC/#variant-calling","title":"Variant calling","text":"<p>A variant call is a conclusion that there is a nucleotide difference vs. some reference at a given position in an individual genome or transcriptome, often referred to as a Single Nucleotide Variant (SNV). The call is usually accompanied by an estimate of variant frequency and some measure of confidence. Similar to other steps in this workflow, there are a number of tools available for variant calling. In this workshop we will be using <code>bcftools</code>, but there are a few things we need to do before actually calling the variants.</p>"},{"location":"1_DesigningVariantC/#step-1-calculate-the-read-coverage-of-positions-in-the-genome","title":"Step 1: Calculate the read coverage of positions in the genome","text":"<p>Do the first pass on variant calling by counting read coverage with <code>bcftools</code>. We will use the command mpileup. The flag -O b tells bcftools to generate a bcf format output file, -o specifies where to write the output file, and -f flags the path to the reference genome:</p> <p><pre><code>module load BCFtools/1.13-GCC-9.2.0\n</code></pre> <pre><code>$ bcftools mpileup -O b -o results/bcf/SRR2584866_raw.bcf -f ref_genome/ecoli_rel606.fasta results/bam/SRR2584866.aligned.sorted.bam\n[mpileup] 1 samples in 1 input files\n[mpileup] maximum number of reads per input file set to -d 250\n</code></pre> We have now generated a file with coverage information for every base.</p>"},{"location":"1_DesigningVariantC/#step-2-detect-the-single-nucleotide-variants-snvs","title":"Step 2: Detect the single nucleotide variants (SNVs)","text":"<p>Identify SNVs using bcftools call. We have to specify ploidy with the flag <code>--ploidy</code>, which is one for the haploid E. coli. -m allows for multiallelic and rare-variant calling, -v tells the program to output variant sites only (not every site in the genome), and -o specifies where to write the output file:</p> <pre><code>bcftools call --ploidy 1 -m -v -o results/vcf/SRR2584866_variants.vcf results/bcf/SRR2584866_raw.bcf </code></pre>"},{"location":"1_DesigningVariantC/#step-3-filter-and-report-the-snv-variants-in-variant-calling-format-vcf","title":"Step 3: Filter and report the SNV variants in variant calling format (VCF)","text":"<p>Filter the SNVs for the final output in VCF format, using <code>vcfutils.pl</code>: <pre><code>vcfutils.pl varFilter results/vcf/SRR2584866_variants.vcf &gt; results/vcf/SRR2584866_final_variants.vcf\n</code></pre></p>"},{"location":"1_DesigningVariantC/#explore-the-vcf-format","title":"Explore the VCF format:","text":"<p>At this stage you can use various tools to analyse the vcf file. Exploring the vcf is beyond the scope of this workshop.</p> <p>Now we are ready for the Next Lesson to put all these commands in a script.</p>"},{"location":"2_AutomaticVariantC/","title":"Automating a Variant Calling Workflow","text":"<p>Aim</p> <ul> <li>Put all the steps from the previous lesson into a script.</li> </ul>"},{"location":"2_AutomaticVariantC/#variant-calling-workflow","title":"Variant calling workflow","text":"<p>Remember our variant calling workflow has the following steps:</p> <ul> <li>Index the reference genome for use by bwa and samtools.</li> <li>Align reads to reference genome.</li> <li>Convert the format of the alignment to sorted BAM, with some intermediate steps.</li> <li>Calculate the read coverage of positions in the genome.</li> <li>Detect the single nucleotide variants (SNVs).</li> <li>Filter and report the SNVs in VCF (variant calling format).</li> </ul> <p>Let's start with creating a new directory as our script working space and copy all the required resources.</p> <p>script</p> <pre><code>$ pwd\n/home/[Your_Username]/scripting_workshop\n\n$ mkdir script_workspace\n\n$ cd script_workspace\n\n$ cp -r /nesi/project/nesi02659/scripting_workshop/variant_calling/* .\n\n$ ls\nref_genome  trimmed_reads\n</code></pre> <p>Now we are ready to start building the script.</p> <p>script</p> <pre><code>$ nano variant_calling.sh\n</code></pre> <p>In the text editor, type the commands</p> <p>script</p> <pre><code>#!/bin/bash \n# Jane Doe\n# 05 March 2022\n# This script runs the variant calling pipeline from mapping to vcf.\nset -e\n# Load all the required modules\nmodule purge\nmodule load BWA/0.7.17-GCC-9.2.0\nmodule load SAMtools/1.13-GCC-9.2.0\nmodule load BCFtools/1.13-GCC-9.2.0\n\n# create the results directories\nmkdir -p results/sam results/bam results/bcf results/vcf\n\n# indexing the genome\ngenome=ref_genome/ecoli_rel606.fasta\nbwa index $genome\n# create a loop that map reads to the genome, sort the bam files and call variants\nfor fq1 in trimmed_reads/*_1.trim.sub.fastq\n    do\necho \"working with file $fq1\"\nbase=$(basename $fq1 _1.trim.sub.fastq)\necho \"base name is $base\"\n# setting the variables\nfq1=trimmed_reads/${base}_1.trim.sub.fastq\n   fq2=trimmed_reads/${base}_2.trim.sub.fastq\n   sam=results/sam/${base}.aligned.sam\n   bam=results/bam/${base}.aligned.bam\n   sorted_bam=results/bam/${base}.aligned.sorted.bam\n   raw_bcf=results/bcf/${base}_raw.bcf\n   variants=results/vcf/${base}_variants.vcf\n   final_variants=results/vcf/${base}_final_variants.vcf\n\n# running the analysis steps\nbwa mem $genome $fq1 $fq2 &gt; $sam\nsamtools view -S -b $sam &gt; $bam\nsamtools sort -o $sorted_bam $bam\nsamtools index $sorted_bam\nbcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam\nbcftools call --ploidy 1 -m -v -o $variants $raw_bcf\nvcfutils.pl varFilter $variants &gt; $final_variants\ndone\n</code></pre> <p>Running the script</p> <p>script</p> <pre><code>$ bash ./variant_calling.sh\n</code></pre> <p>This should take about 10 minutes.</p> <p>Adding executable permissions</p> <p>The way the script is written means we have to indicate which program to use whenever we are running it.  So to run without calling bash, we can change the script permissions.</p> <p>script</p> <pre><code>$ ls -l variant_calling.sh -rw-rw-r-- 1 fayfa80p fayfa80p 1401 Mar  5 22:29 variant_calling.sh\n\n$ chmod u+x variant_calling.sh\n\n$ ls -l variant_calling.sh -rwxrw-r-- 1 fayfa80p fayfa80p 1401 Mar  5 22:29 variant_calling.sh\n# note colour change on the script filename\n</code></pre> <p>Now we can execute the script without calling bash <pre><code>$ ./variant_calling.sh\n</code></pre></p> <p>In the Next Lesson we will now prepare the script to run on the HPC environment</p>"},{"location":"3_RNAseq/","title":"RNA-seq Mapping And Count Data Workflow","text":"<p>This material is extracted from the RNA-seq workshop Lesson</p> <p>Objectives and overall workflow</p> <ul> <li>To develop a pipeline that does mapping and count the number of reads that mapped then overall put all these steps into a script.</li> <li>Understand and perform the steps involved in RNA-seq mapping and read count.</li> <li>Use command line tools to run the pipeline.</li> </ul> <p> </p>"},{"location":"3_RNAseq/#assumptions","title":"Assumptions","text":"<ul> <li>You have already performed trimming and filtering of your reads and saved in a directory called trimmed_reads.</li> <li>You have a reference genome saved in a directory called ref_genome.</li> </ul> <p>In this workshop, we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are:</p> <pre><code>$ cd ~/scripting_workshop\n\n$ pwd\n/home/[your_username]/scripting_workshop\n# good I am ready to work\n</code></pre> <p>Checking to make sure we have the directory and files for the workshop.</p> <pre><code>$ ls\nrna_seq  variant_calling scheduler\n</code></pre> <p>hint : If you do not have the workshop directory, you can copy it using the command: <code>cp -r  /nesi/project/nesi02659/scripting_workshop/ ~</code> </p> <pre><code>$ cd rna_seq\n\n$ ls\nref_genome  trimmed_reads </code></pre>"},{"location":"3_RNAseq/#alignment-to-a-reference-genome","title":"Alignment to a reference genome","text":"<p>RNA-seq generate gene expression information by quantifying the number of transcripts (per gene) in a sample. This is acompished by counting the number of transcripts that have been sequenced - the more active a gene is, the more transcripts will be in a sample, and the more reads will be generated from that transcript.</p> <p>For RNA-seq, we need to align or map each read back to the genome, to see which gene produced it. - Highly expressed genes will generate lots of transcripts, so there will be lots of reads that map back to the position of that transcript in the genome. - The per-gene data we work with in an RNA-seq experiment are counts: the number of reads from each sample that originated from that gene.</p>"},{"location":"3_RNAseq/#preparation-of-the-genome","title":"Preparation of the genome","text":"<p>To be able to map (align) sequencing reads on the genome, the genome needs to be indexed first. In this workshop we will use HISAT2.</p> <p>script</p> <pre><code>$ cd ~/scripting_workshop/rna_seq/ref_genome\n\n#to list what is in your directory:\n$ ls ~/scripting_workshop/rna_seq/ref_genome\nSaccharomyces_cerevisiae.R64-1-1.99.gtf  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa\n\n$ module load HISAT2/2.2.0-gimkl-2020a\n\n# index file:\n$ hisat2-build -p 4 -f Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa Saccharomyces_cerevisiae.R64-1-1.dna.toplevel\n\n#list what is in the directory:\n$ ls ~/scripting_workshop/rna_seq/ref_genome\nSaccharomyces_cerevisiae.R64-1-1.99.gtf              Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.4.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.8.ht2\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.1.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.5.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.2.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.6.ht2\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.3.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.7.ht2   </code></pre> <p>Arguments:  * -p number of threads  * -f fasta file</p> <p>How many files were created during the indexing process?</p>"},{"location":"3_RNAseq/#alignment-on-the-genome","title":"Alignment on the genome","text":"<p>Now that the genome is prepared. Sequencing reads can be aligned.</p> <p>Information required:</p> <ul> <li>Where the sequence information is stored (e.g. fastq files ...) ?</li> <li>What kind of sequencing: Single End or Paired end ?</li> <li>Where are stored the indexes and the genome? </li> <li> <p>Where will the mapping files be stored?</p> </li> <li> <p>Now, lets move one folder up (into the rna_seq folder):</p> </li> </ul> <pre><code>$ cd ..\n\n$ ls\nref_genome  trimmed_reads\n</code></pre> <p>Let's map one of our sample to the reference genome</p> <p>script</p> <pre><code>$ pwd\n/home/[Your_Username]/scripting_workshop/rna_seq/\n\n$ mkdir Mapping\n\n$ ls\nref_genome  Mapping  trimmed_reads\n</code></pre> <p>let's use a for loop to process our samples:</p> <p>script</p> <p><pre><code>$ cd trimmed_reads\n\n$ ls\nSRR014335-chr1.fastq  SRR014336-chr1.fastq  SRR014337-chr1.fastq  SRR014339-chr1.fastq  SRR014340-chr1.fastq  SRR014341-chr1.fastq\n</code></pre> <pre><code>for filename in *\n do\nbase=$(basename ${filename} .fastq)\nhisat2 -p 4 -x ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S ../Mapping/${base}.sam --summary-file ../Mapping/${base}_summary.txt\ndone\n</code></pre></p> <p>Arguments:  * -x The basename of the index for the reference genome.   * -U Comma-separated list of files containing unpaired reads to be aligned  * -S File to write SAM alignments to. By default, alignments are written to the \u201cstandard out\u201d or \u201cstdout\u201d filehandle  </p> <p>Now we can explore our SAM files.</p> <p>script</p> <pre><code>$ cd ../Mapping\n\n$ ls\nSRR014335-chr1.sam          SRR014336-chr1_summary.txt  SRR014339-chr1.sam          SRR014340-chr1_summary.txt\nSRR014335-chr1_summary.txt  SRR014337-chr1.sam          SRR014339-chr1_summary.txt  SRR014341-chr1.sam\nSRR014336-chr1.sam          SRR014337-chr1_summary.txt  SRR014340-chr1.sam          SRR014341-chr1_summary.txt\n</code></pre>"},{"location":"3_RNAseq/#converting-sam-files-to-bam-files","title":"Converting SAM files to BAM files","text":"<p>The SAM file, is a tab-delimited text file that contains information for each individual read and its alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification.</p> <p>The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file.</p>"},{"location":"3_RNAseq/#a-quick-look-into-the-sam-file","title":"A quick look into the sam file","text":"<pre><code>$ less SRR014335-chr1.sam The file begins with a header, which is optional. The header is used to describe the source of data, reference sequence, method of alignment, etc., this will change depending on the aligner being used. Following the header is the alignment section. Each line that follows corresponds to alignment information for a single read. Each alignment line has 11 mandatory fields for essential mapping information and a variable number of other fields for aligner specific information. An example entry from a SAM file is displayed below with the different fields highlighted.\n</code></pre> <p>We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (<code>-S</code>) and to output BAM format (<code>-b</code>):</p> <p>script</p> <p><pre><code>module load SAMtools/1.10-GCC-9.2.0\n</code></pre> <pre><code>for filename in *.sam\n do\nbase=$(basename ${filename} .sam)\nsamtools view -S -b ${filename} -o ${base}.bam\ndone\n</code></pre> <pre><code>$ ls\nSRR014335-chr1.bam  SRR014336-chr1.bam  SRR014337-chr1.bam  SRR014339-chr1.bam  SRR014340-chr1.bam  SRR014341-chr1.bam\nSRR014335-chr1.sam  SRR014336-chr1.sam  SRR014337-chr1.sam  SRR014339-chr1.sam  SRR014340-chr1.sam  SRR014341-chr1.sam\n</code></pre></p> <p>Next we sort the BAM file using the sort command from samtools. <code>-o</code> tells the command where to write the output.</p> <p>Note</p> <p>SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input.**</p> <p>script</p> <pre><code>for filename in *.bam\n do\nbase=$(basename ${filename} .bam)\nsamtools sort -o ${base}_sorted.bam ${filename}\ndone\n</code></pre> <p>We can use samtools to learn more about the bam file as well.</p>"},{"location":"3_RNAseq/#some-stats-on-your-mapping","title":"Some stats on your mapping:","text":"<p>script</p> <pre><code>$ samtools flagstat SRR014335-chr1_sorted.bam 156984 + 0 in total (QC-passed reads + QC-failed reads)\n31894 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n136447 + 0 mapped (86.92% : N/A)\n0 + 0 paired in sequencing\n0 + 0 read1\n0 + 0 read2\n0 + 0 properly paired (N/A : N/A)\n0 + 0 with itself and mate mapped\n0 + 0 singletons (N/A : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre>"},{"location":"3_RNAseq/#read-summarization","title":"Read Summarization","text":"<p>Sequencing reads often need to be assigned to genomic features of interest after they are mapped to the reference genome. This process is often called read summarization or read quantification. Read summarization is required by a number of downstream analyses such as gene expression analysis and histone modification analysis. The output of read summarization is a count table, in which the number of reads assigned to each feature in each library is recorded.</p>"},{"location":"3_RNAseq/#counting","title":"Counting","text":"<ul> <li>We need to do some counting!</li> <li>Want to generate count data for each gene (actually each exon) - how many reads mapped to each exon in the genome, from each of our samples?</li> <li>Once we have that information, we can start thinking about how to determine which genes were differentially expressed in our study.</li> </ul>"},{"location":"3_RNAseq/#subread-and-featurecounts","title":"Subread and FeatureCounts","text":"<ul> <li>The featureCounts tool from the Subread package can be used to count how many reads aligned to each genome feature (exon).</li> <li>Need to specify the annotation informatyion (.gtf file)  You can process all the samples at once:</li> </ul> <p>script</p> <pre><code>$ cd ~/scripting_workshop/rna_seq\n\n$ module load Subread/2.0.0-GCC-9.2.0\n\n$ pwd\n/home/[Your_Username]/scripting_workshop/rna_seq\n\n$ mkdir Counts\n\n$ cd Counts\n\n$ featureCounts -a ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o ./yeast_counts.txt -T 2 -t exon -g gene_id ../Mapping/*sorted.bam\n</code></pre> <p>Arguments:</p> <ul> <li>-a Name of an annotation file. GTF/GFF format by default</li> <li>-o Name of output file including read counts</li> <li>-T Specify the number of threads/CPUs used for mapping. 1 by default</li> <li>-t Specify feature type in GTF annotation. 'exon' by default. Features used for read counting will be extracted from annotation using the provided value.</li> <li>-g Specify attribute type in GTF annotation. 'gene_id' by default. Meta-features used for read counting will be extracted from annotation using the provided value.</li> </ul>"},{"location":"3_RNAseq/#group-exercise","title":"Group Exercise","text":"<p>Now, let's work together in our groups to create an RNA-seq mapping and count script.</p> <p>at this stage we have mastered the art of writing scripts, instead of running them on the commandline, let us now run them on HPC.</p>"},{"location":"4_IntroductiontoHPC/","title":"Introduction to HPC","text":""},{"location":"4_IntroductiontoHPC/#defining-high-performance-computing","title":"Defining high-performance computing","text":"<p>The simplest way of defining high-performance computing is by saying that it is the using of high-performance computers (HPC). However, this leads to our next question what is a HPC .</p> <p>A high-performance computer is a network of computers in a cluster that typically share a common purpose and are used to accomplish tasks that might otherwise be too big for any one computer.</p> <p>While modern computers can do a lot (and a lot more than their equivalents 10-20 years ago), there are limits to what they can do and the speed at which they are able to do this. One way to overcome these limits is to pool computers together to create a cluster of computers. These pooled resources can then be used to run software that requires more total memory, or need more processors to complete in a reasonable time.</p> <p>One way to do this is to take a group of computers and link them together via a network switch. Consider a case where you have five 4-core computers. By connecting them together, you could run jobs on 20 cores, which could result in your software running faster.</p>"},{"location":"4_IntroductiontoHPC/#hpc-architectures","title":"HPC architectures","text":"<p>Most HPC systems follow the ideas described above of taking many computers and linking them via network switches. What distinguishes a high-performance computer from the computer clusters described above is:</p> <ul> <li>The number of computers/nodes </li> <li>The strength of each individual computer/node </li> <li>The network interconnect \u2013 this dictates the communication speed between nodes. The faster this speed is, the more a group of individual nodes will act like a unit.</li> </ul>"},{"location":"4_IntroductiontoHPC/#nesi-mahuika-cluster-architecture","title":"NeSI Mahuika Cluster architecture","text":"<p>NeSI Mahuika cluster (CRAY HPE CS400) system consists of a number of different node types. The ones visible to researchers are:</p> <ul> <li>Login nodes</li> <li>Compute nodes</li> </ul> <p> </p> <p> </p> <p>In reality</p> <p> </p> <p>Back to homepage</p>"},{"location":"5_working_with_job_scheduler/","title":"Working with job scheduler","text":""},{"location":"5_working_with_job_scheduler/#introduction-to-slurm-scheduler-and-directives","title":"Introduction to slurm scheduler and directives","text":"<p>An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when. In brief, scheduler is a </p> <ul> <li>Mechanism to control access by many users to shared computing resources</li> <li>Queuing / scheduling system for users\u2019 jobs</li> <li>Manages the reservation of resources and job execution on these resources </li> <li>Allows users to \u201cfire and forget\u201d large, long calculations or many jobs (\u201cproduction runs\u201d)</li> </ul> <p>Commonly used schedulers</p> <ul> <li>To ensure the machine is utilised as fully as possible</li> <li>To ensure all users get a fair chance to use compute resources (demand usually exceeds supply)</li> <li>To track usage - for accounting and budget control</li> <li>To mediate access to other resources e.g. software licences</li> </ul> <p>Commonly used schedulers</p> <ul> <li>Slurm</li> <li>PBS , Torque</li> <li>Grid Engine</li> </ul> <p> </p> <p>All NeSI clusters use Slurm (Simple Linux Utility for Resource Management) scheduler (or job submission system) to manage resources and how they are made available to users. The main commands you will use with Slurm on NeSI Mahuika cluster are:</p> <p>A quick note on <code>sinfo</code>(Query the current state of nodes) which is not a command a researcher will use regularly but helps HPC admins and support staff with monitoring.</p>"},{"location":"5_working_with_job_scheduler/#life-cycle-of-a-slurm-job","title":"Life cycle of a slurm job","text":"Command Function <code>sbatch</code> Submit non-interactive (batch) jobs to the scheduler <code>squeue</code> List jobs in the queue <code>scancel</code> Cancel a job <code>sacct</code> Display accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database <code>srun</code> Slurm directive for parallel computing <code>sinfo</code> Query the current state of nodes <code>salloc</code> Submit interactive jobs to the scheduler Exercise 5.1 <ul> <li> <p>summary of current states of compute nodes known to the scheduler <pre><code>sinfo\n</code></pre></p> </li> <li> <p>similar to above but expanded <pre><code>sinfo --format=\"%16P %.8m %.5a %10T %.5D %80N\"\n</code></pre></p> </li> <li> <p>will print a long output as it is one row per compute node in the cluster <pre><code>sinfo -N -l\n</code></pre></p> </li> <li> <p>Explore the capacity of a compute node  <pre><code>sinfo -n wch001 -o \"%n %c %m\"\n</code></pre></p> </li> </ul>"},{"location":"5_working_with_job_scheduler/#anatomy-of-a-slurm-script-and-submitting-first-slurm-job","title":"Anatomy of a slurm script and submitting first slurm job \ud83e\uddd0","text":"<p>As with most other scheduler systems, job submission scripts in Slurm consist of a header section with the shell specification and options to the submission command (<code>sbatch</code> in this case) followed by the body of the script that actually runs the commands you want. In the header section, options to <code>sbatch</code> should be prepended with <code>#SBATCH</code>.</p> <p> </p> <p>Commented lines <code>#</code></p> <p>Commented lines are ignored by the bash interpreter, but they are not ignored by slurm. The <code>#SBATCH</code> parameters are read by slurm when we submit the job. When the job starts, the bash interpreter will ignore all lines starting with <code>#</code>. This is very similar to the shebang mentioned earlier, when you run your script, the system looks at the <code>#!</code>, then uses the program at the subsequent path to interpret the script, in our case <code>/bin/bash</code> (the program <code>bash</code> found in the /bin directory</p> header use description --job-name <code>#SBATCH --job-name=MyJob</code> The name that will appear when using squeue or sacct. --account <code>#SBATCH --account=nesi12345</code> The account your core hours will be 'charged' to. --time <code>#SBATCH --time=DD-HH:MM:SS</code> Job max walltime. --mem <code>#SBATCH --mem=512MB</code> Memory required per node. --cpus-per-task <code>#SBATCH --cpus-per-task=10</code> Will request 10 logical CPUs per task. --output <code>#SBATCH --output=%j_output.out</code> Path and name of standard output file. <code>%j</code> will be replaced by the job ID. --mail-user <code>#SBATCH --mail-user=me23@gmail.com</code> address to send mail notifications. --mail-type <code>#SBATCH --mail-type=ALL</code> Will send a mail notification at BEGIN END FAIL. <code>#SBATCH --mail-type=TIME_LIMIT_80</code> Will send message at 80% walltime. Exercise 5.2 <p>Let's put these directives together and compile our first slurm script</p> <ul> <li>First create a new working directory and end the directory</li> </ul> <pre><code> cd ~/scripting_workshop/scheduler\n</code></pre> <ul> <li> <p>confirm the path is correct  <pre><code>pwd\n</code></pre></p> </li> <li> <p>create a new directory for this section and change the directory to it - Check for the follow up not <code>&amp;&amp;</code> <pre><code>mkdir ex_5.2 &amp;&amp; cd ex_5.2\n</code></pre></p> </li> <li> <p>use a text editor of choice to create a file named firstslurm.sl - we will use nano here <pre><code>nano firstslurm.sl\n</code></pre></p> </li> <li> <p>Content of <code>firstslurm.sl</code> should be as below. Please discuss as you make progress</p> </li> </ul> <pre><code>#!/bin/bash \n#SBATCH --job-name      myfirstslurmjob\n#SBATCH --account       nesi02659\n#SBATCH --time          00:01:00\n#SBATCH --cpus-per-task 1\n#SBATCH --mem           512\n#SBATCH --output        slurmjob.%j.out\nsleep 200\necho \"I am a slurm job and I slept for 200 seconds\"\necho \"$SLURM_JOB_ID END\"\n</code></pre> <ul> <li>Save and Exit</li> <li>Submit the script with <code>sbatch</code> command</li> </ul> <p><pre><code>sbatch firstslurm.sl\n</code></pre> *  Execute <code>squeue --me</code> and <code>sacct</code>. Discuss the outputs .i.e.</p> <p><pre><code>squeue --me\n</code></pre> <pre><code>sacct\n</code></pre></p> <p>The meaning of <code>&amp;&amp;</code> and <code>&amp;</code> are intrinsically different.</p> <ul> <li>What is <code>&amp;&amp;</code> in Bash? In Bash\u2014and many other programming languages\u2014<code>&amp;&amp;</code> means \u201cAND\u201d. And in command execution context like this, it means items to the left as well as right of &amp;&amp; should be run in sequence in this case.</li> <li>What is &amp; in Bash? And a single <code>&amp;</code>means that the preceding commands\u2014to the immediate left of the &amp;\u2014should simply be run in the background.</li> </ul>"},{"location":"5_working_with_job_scheduler/#stdoutstderr-from-jobs","title":"STDOUT/STDERR from jobs","text":"<ul> <li>STDOUT - your process writes conventional output to this file handle</li> <li>STDERR - your process writes diagnostic output to this file handle.</li> </ul> <p>STDOUT and STDERR from jobs are, by default, written to a file called <code>slurm-JOBID.out</code> and <code>slurm-JOBID.err</code> in the working directory for the job (unless the job script changes this, this will be the directory where you submitted the job). So for a job with ID 12345 STDOUT and STDERR will be <code>slurm-12345.out</code> and <code>slurm-12345.err</code>.</p> <p>When things go wrong, first step of debugging (STORY TIME !) starts with a referral to these files. </p>"},{"location":"5_working_with_job_scheduler/#assessing-resource-utilisation-cpu-memory-time","title":"Assessing resource utilisation (cpu, memory, time)","text":"<p>Understanding the resources you have available and how to use them most efficiently is a vital skill in high performance computing. The three resources that every single job submitted on the platform needs to request are:</p> <ul> <li>CPUs (i.e. logical CPU cores), and</li> <li>Memory (RAM), and</li> <li>Time.</li> </ul> <p>What happens if I ask for the wrong resources?</p> Resource Asking for too much Not asking for enough Number of CPUs Job may wait in the queue for longer Job will run more slowly than expected, and so may run out time Drop in fairshare score which determines job priority Memory (above) Job will fail, probably with <code>OUT OF MEMORY</code> error, segmentation fault or bus error Wall time (above) Job will run out of time and get killed Exercise 5.3 <p>Let's submit another slurm job and review its resource utilisation</p> <ul> <li>Change the working directory to Exercise_5.3</li> </ul> <pre><code>cd ~/scripting_workshop/scheduler/ex_5.3\n</code></pre> <ul> <li>Run <code>ls</code> command and you should see two files (one .R and one sl) and one directory named slurmout <pre><code> ls -F\n</code></pre> <pre><code>bowtie-test.sl*  input_data/  slurmout/\n</code></pre></li> <li> <p>Review the slurm script bowtie-test.sl with nano and edit the corresponding sections (hint :email) <pre><code>sbatch bowtie-test.sl </code></pre></p> </li> <li> <p>use <code>squeue --me</code> and <code>sacct</code> again to evaluate the job status</p> </li> <li> <p>Once the job ran into completion, use <code>nn_seff JOBID</code> command to print the resource utilisation statistics (Replace JOBID with the corresponding number)</p> </li> </ul> <pre><code>$ nn_seff 25222190\nJob ID: 25222190\nCluster: mahuika\nUser/Group: me1234/me1234\nState: COMPLETED (exit code 0)\nCores: 1\nTasks: 1\nNodes: 1\nJob Wall-time:  18.33%  00:00:33 of 00:03:00 time limit\nCPU Efficiency: 93.94%  00:00:31 of 00:00:33 core-walltime\nMem Efficiency: 1.33%  13.62 MB of 1.00 GB\n</code></pre> <p>Now review the content of <code>.err</code> and <code>.out</code> files in /slurmout directory </p> <p>Feeling adventurous ? - Refer to Supplementary material on slurm profiling</p>"},{"location":"5_working_with_job_scheduler/#compiling-slurm-scripts-for-variant-calling-and-rna-seq-episodes","title":"Compiling slurm scripts for Variant Calling and RNA-seq episodes","text":"Exercise 5.4 \ud83d\ude2c <p>Purpose of this exercise is to compile a slurm submission script based on the script we wrote in episode 2 - Automating variant calling workflow</p> <ul> <li>recommend creating a new directory for the exercise .i.e <code>ex_5.4</code></li> <li>Name of the file is <code>variant_calling.sl</code> (note that we have change the extension from <code>.sh</code> to <code>.sl</code>)</li> <li> <p>In terms of slurm variables</p> <ul> <li>name of the job is <code>variant_calling_workflow</code></li> <li>number of CPUS is <code>2</code></li> <li>timelimit <code>15 minutes</code></li> <li>amount of memory in GB <code>4G</code></li> <li>generate  .err files and .out where both should be re-directed to the directory slurmout</li> <li>an email notification at the end of the job </li> </ul> </li> <li> <p>We don't want to replicate input data  in multiple places .i.e. be conservative in-terms how you use research storage</p> </li> <li>Therefore, use the same reference genome file (assign the filename to variable <code>genome</code> and the trimmed read files (assign the path of these files to variable <code>trimmed</code>) used in the first episode</li> </ul> <pre><code>genome=~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta\ntrimmed=~/scripting_workshop/variant_calling/trimmed_reads\n</code></pre> Exercise 5.5 \ud83d\ude2c <ul> <li>Now it's your turn to compile a slurm submission script for the RNA-seq workflow. \ud83d\ude0a</li> </ul> <p>Back to homepage</p>"},{"location":"6_supplementary_1/","title":"S1 : Accessing software via modules","text":"<p>On a high-performance computing system, it is quite rare that the software we want to use is available when we log in. It is installed, but we will need to \u201cload\u201d it before it can run.</p> <p>Before we start using individual software packages, however, we should understand the reasoning behind this approach. The three biggest factors are:</p> <ul> <li>software incompatibilities</li> <li>versioning</li> <li>dependencies</li> </ul> <p>One of the workarounds for this issue is Environment modules. A module is a self-contained description of a software package \u2014 it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages.</p> <p>There are a number of different environment module implementations commonly used on HPC systems and the one used in NeSI Mahuika cluster is <code>Lmod</code> where the <code>module</code> command is used to interact with environment modules.</p> <p>Commonly used <code>module</code> sub-commands</p> <ul> <li> <p>View available modules <pre><code>module avail\n</code></pre></p> </li> <li> <p>View all modules which match the keyword in their name <pre><code>module avail KEYWORD\n</code></pre></p> </li> <li> <p>View all modules which match the keyword in their name or description <pre><code>module spider KEYWORD\n</code></pre></p> </li> <li> <p>Load a specific program</p> <ul> <li>Note: All modules on NeSI have version and toolchain/environment suffixes. If none is specified, the default version for the tool is loaded. The default version can be seen with the module avail command. <pre><code>module load MY_APPLICATION\n</code></pre></li> </ul> </li> <li> <p>Swap a currently loaded module for a different one</p> </li> </ul> <pre><code>module switch CURRENT_MODULE DESIRED_MODULE\n</code></pre> <ul> <li>Unload all current modules</li> </ul> <pre><code>module purge\n</code></pre> <p>Danger</p> <p>Please do not use <code>$ module --force purge</code></p> <p>Back to homepage</p>"},{"location":"7_supplementary_2/","title":"S2 : slurm profiling","text":"<p>Although <code>nn_seff</code> command is a quick and easy way to determine the resource utilisation, it relies on peak values (data gets recorded every 30 seconds) which doesn't allows us to examine resource usage over the run-time of the job. There are number of in-built/external tools to achieve the latter which will require some effort to understand its deployment, tracing and interpretation. Therefore, we will use slurm native profiling to evaluate resource usage over run-time. This is a simple and elegant solution.</p> Exercise S.2.1 <ul> <li> <p>Download and decompress the content <pre><code>wget -c Exercise_S21.tar.gz https://github.com/DininduSenanayake/nesi_introductory_custom/releases/download/v1.0/Exercise_S21.tar.gz -O - | tar -xz\n</code></pre> <pre><code>cd Exercise_S21\n</code></pre></p> </li> <li> <p>Run <code>ls</code> command and you should see three files (one .R,sl and one .py - We will discuss the purpose of this .py file after submitting the job) and one directory named slurmout <pre><code>ls -F\n</code></pre> <pre><code>example1_arraysum.R  example1_arraysum.sl  profile_plot_Jul2020.py  slurmout/\n</code></pre></p> </li> <li> <p>Review the slurm script with cat Or another text editor and submit with sbatch <pre><code>sbatch example1_arraysum.sl </code></pre></p> </li> <li> <p>Do take a note of the JOBID as we are going to need it for next step. Otherwise, we use <code>squeue --me</code> OR <code>sacct</code> command as before to monitor the status</p> </li> <li>Also, you can <code>watch</code> the status of this job via <code>$ watch -n 1 -d \"squeue -j JOBID\"</code>. </li> <li> <p><code>watch</code> command execute a program periodically, showing output fullscreen. Exiting the <code>watch</code> screen by done by pressing <code>Ctrl+x</code> </p> </li> <li> <p>Let's create slurm profile graphs</p> <ul> <li>collate the data into an HDF5 file using the command. Replace JOBID with the corresponding number </li> </ul> <p><pre><code>sh5util -j JOBID\n</code></pre> <pre><code>sh5util: Merging node-step files into ./job_JOBID.h5\n</code></pre></p> <ul> <li>execute the script on .h5 file. We will need one of the Python 3 modules to do this. Ignore the deprecating warning.  <pre><code>module purge </code></pre> <pre><code>module load Python/3.8.2-gimkl-2020a\n</code></pre></li> <li>Replace JOBID with the corresponding number <pre><code>python profile_plot_Jul2020.py job_JOBID.h5\n</code></pre></li> </ul> </li> <li> <p>This should generate a .png file where the filename is in the format of job_23258404_profile.png</p> </li> </ul> <p> </p> <p>Back to homepage</p>"},{"location":"8_supplementary_3/","title":"S3 : Solutions","text":"<p> \u00ab7. Supplementary 2 </p> Exercise 5.4 \ud83d\ude2c <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      variant_calling_workflow\n#SBATCH --cpus-per-task 2\n#SBATCH --time          00:15:00\n#SBATCH --mem           4G\n#SBATCH --output        slurmout/variant_calling-%j.out\n#SBATCH --error         slurmout/variant_calling-%j.err\n#SBATCH --mail-type     END\n#SBATCH --mail-user     myemail@email.co.nz\n# Load all the required modules\nmodule purge\nmodule load BWA/0.7.17-GCC-9.2.0\nmodule load SAMtools/1.13-GCC-9.2.0\nmodule load BCFtools/1.13-GCC-9.2.0\n\necho \"$PWD\"\n# create the results directories\nmkdir -p results/sam results/bam results/bcf results/vcf\n\n# indexing the genome\ngenome=~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta\ntrimmed=~/scripting_workshop/variant_calling/trimmed_reads\nbwa index $genome\n# create a loop that map reads to the genome, sort the bam files and call variants\nfor fq1 in ${trimmed}/*_1.trim.sub.fastq\n do\necho \"working with file $fq1\"\nbase=$(basename $fq1 _1.trim.sub.fastq)\necho \"base name is $base\"\n# setting the variables\nfq1=${trimmed}/${base}_1.trim.sub.fastq\n    fq2=${trimmed}/${base}_2.trim.sub.fastq\n    sam=results/sam/${base}.aligned.sam\n    bam=results/bam/${base}.aligned.bam\n    sorted_bam=results/bam/${base}.aligned.sorted.bam\n    raw_bcf=results/bcf/${base}_raw.bcf\n    variants=results/vcf/${base}_variants.vcf\n    final_variants=results/vcf/${base}_final_variants.vcf\n\n# running the analysis steps\nbwa mem $genome $fq1 $fq2 &gt; $sam\nsamtools view -S -b $sam &gt; $bam\nsamtools sort -o $sorted_bam $bam\nsamtools index $sorted_bam\nbcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam\nbcftools call --ploidy 1 -m -v -o $variants $raw_bcf\nvcfutils.pl varFilter $variants &gt; $final_variants\ndone\necho \"DONE\"\n</code></pre> Exercise 5.5 \ud83d\ude2c <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi02659\n#SBATCH --job-name      rna-seq_workflow\n#SBATCH --cpus-per-task 2\n#SBATCH --time          00:15:00\n#SBATCH --mem           4G\n#SBATCH --output        rna-seq_workflow-%j.out\n#SBATCH --error         rna-seq_workflow-%j.err\n#SBATCH --mail-type     END\n#SBATCH --mail-user     myemail@email.org.nz\necho \"$PWD\"\nmkdir -p ~/scripting_workshop/scheduler/ex_5.5/{Mapping,Counts} &amp;&amp; cd ~/scripting_workshop/scheduler/ex_5.5/\ncp -r /nesi/project/nesi02659/scripting_workshop/rna_seq/* ./\n\nmodule purge\nmodule load HISAT2/2.2.0-gimkl-2020a\nmodule load SAMtools/1.10-GCC-9.2.0\nmodule load Subread/2.0.0-GCC-9.2.0\n\necho $PWD\n#index file\nhisat2-build -p 4 -f $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel\n\n#Mapping Samples to the reference genome\nfor filename in $PWD/trimmed_reads/*\n  do\nbase=$(basename ${filename} .fastq)\nhisat2 -p 4 -x $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S $PWD/Mapping/${base}.sam --summary-file $PWD/Mapping/${base}_summary.txt\n  done\n#Convert SAMfiles to BAM\nfor filename in $PWD/Mapping/*.sam\n  do\nbase=$(basename ${filename} .sam)\nsamtools view -S -b ${filename} -o $PWD/Mapping/${base}.bam\n  done\n#Sort BAM files\nfor filename in $PWD/Mapping/*.bam\n  do\nbase=$(basename ${filename} .bam)\nsamtools sort -o $PWD/Mapping/${base}_sorted.bam ${filename}\ndone\n#count how many reads aligned to each genome feature (exon).\nfeatureCounts -a $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o $PWD/Counts/yeast_counts.txt -T 2 -t exon -g gene_id $PWD/Mapping/*sorted.bam\n</code></pre> <p>Back to homepage</p>"}]}
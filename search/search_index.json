{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#introduction-to-bash-scripting-and-hpc-scheduler","title":"Introduction to Bash Scripting and HPC Scheduler","text":"<p>Prerequisites</p> <ul> <li> Familiarity with terminal and basic linux commands</li> <li> Some knowledge on shell environment variables and <code>for</code> loops</li> <li> Ability to use a terminal based text editor such as <code>nano</code> <ul> <li> This is not much of an issue as we are using JupyterHub which has a more friendlier text editor.  </li> </ul> </li> <li> Intermediate level knowledge on Molecular Biology and Genetics</li> </ul> <p>Recommended but not required</p> <ul> <li> Attend Genomics Data Carpentry and RNA-Seq Data Analysis workshops</li> </ul> <p></p> <p>Some of the things we won't cover in this workshop</p> <ul> <li>Domain specific concepts in<ul> <li>Genetics, Genomics and DNA Sequencing </li> <li>Variant Calling (covers in Genomics Data Carpentry Workshop)</li> <li>RNA sequencing and data analysis ( covers in RNA-Seq Data Analysis Workshop)</li> </ul> </li> </ul> <p></p> <p>Setup</p> <p>Workshop material is designed to run on NeSI Mahuika cluster via Jupyter. Instructions on how to Set/Reset Authentication factors to access NeSI Services and Jupyter Login instructions can be found here</p> <p></p> <p>Content</p> Lesson Overview Background  Objective of this workshop 1. Designing a Variant Calling Workflow   Develop and test the steps involved in calling variants 2. Automating a Variant Calling Workflow   Compile a script based on the above steps 3. RNA-Seq Mapping And Count Data Workflow   Develop, test &amp; compile a script for mapping reads and counting transcripts in a RNA-Seq data analysis pipeline 4. Introduction to HPC   Introduction to High Performance Computing 5. Working with Job Scheduler   Introduction to HPC Job Schedulers, Slurm Scheduler &amp; life cycle of a Slurm job, Assessing resource utilisation and profiling 6. Supplementary #1 7. Supplementary #2 8. Supplementary #3"},{"location":"0_Background/","title":"Background","text":"<p>Primary objective of this workshop</p> <ul> <li>Illustrate the step by step process for developing a Pipeline/Workflow<ul> <li>Developing a Pipeline/Workflow starting from \"Interactive\" testing/debugging TO \"Scripts\" for better handling and make the process more reproducible TO \"Scaling on Clusters with Scripts with variables for Schedulers\"</li> </ul> </li> </ul> <p></p>"},{"location":"1_DesigningVariantC/","title":"Variant Calling Workflow","text":"<p>This material is extracted from the Genomics Data Carpentry Lesson</p> <p>Objectives and overall workflow</p> <ul> <li>Understand and perform the steps involved in variant calling.</li> <li>Describe the types of data formats encountered during variant calling.</li> <li>Use command line tools to perform variant calling.</li> </ul> <p> </p> <p>Assumptions</p> <ul> <li>You have already performed trimming and filtering of your reads and saved them in a directory called <code>trimmed_reads</code>.</li> <li>You have a reference genome saved in a directory called <code>ref_genome</code>.</li> </ul> <p>In this workshop we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are:</p> <p>script</p> <p><pre><code>cd ~\n</code></pre> <pre><code>pwd\n</code></pre> Output:  <code>/home/$USER</code></p> <ul> <li>You should see your own username in place of <code>$USER</code></li> <li>FYI: <code>$USER</code> is not an arbitrary string as it is a real environment variable. Run <code>echo $USER</code> and see what happens </li> </ul> <p>Checking to make sure we have the directory and files for the workshop.</p> <pre><code>ls\n</code></pre> <ul> <li>You should see a directory named <code>/scripting_workshop</code></li> </ul> <p>Quick Check</p> <p>If you do not have the workshop directory, you can copy it using the command: <code>cp -r  /nesi/project/nesi02659/scripting_workshop/ ~</code> </p> <p>script</p> <p><pre><code>cd scripting_workshop/variant_calling\n</code></pre> <pre><code>ls\n</code></pre> Output:  <code>ref_genome  trimmed_reads</code> </p>"},{"location":"1_DesigningVariantC/#alignment-to-a-reference-genome","title":"Alignment to a reference genome","text":"<p>First we need to create directories for the results that will be generated as part of this workflow. We can do this in a single line of code, because mkdir can accept multiple new directory names as input.</p> <p>script</p> <pre><code>mkdir -p results/sam results/bam results/bcf results/vcf\n</code></pre> <ul> <li>Another quick and easy way to create multiple directories reside within the same parent directory is to wrap them with <code>{}</code> (comma separated) e.g.,  <pre><code>mkdir -p results/{sam,bam,bcf,vcf}\n</code></pre></li> </ul>"},{"location":"1_DesigningVariantC/#index-the-reference-genome","title":"Index the reference genome","text":"<p>Our first step is to index the reference genome for use by BWA. Indexing allows the aligner to quickly find potential alignment sites for query sequence in a genome, which saves time during alignment. Indexing the reference only has to be run once. The only reason you would want to create a new index is if you are working with a different reference genome or you are using a different tool for alignment.</p> <p>Since we are working on the NeSI HPC, we need to search and load the package before we start using it.</p> <p>Software as modules</p> <ul> <li>Similar to other HPCs/SuperComputers, NeSI Clusters provide software as modules (this is not the only way to deploy software as it can be done via other means such as conda, containers, etc.).</li> <li>A module is a self-contained description of a software package \u2014 it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages.</li> <li>Refer to supplementary 1 - Accessing software via modules for more information. </li> </ul> <p>script</p> <p><pre><code>#Search for a module\nmodule spider bwa\n</code></pre> <pre><code>#Load BWA module\nmodule purge\nmodule load BWA/0.7.17-GCC-9.2.0\n</code></pre></p> <p>All-In-One</p> <p>We will be needing a few modules for this episode and the RNA-Seq Mapping episode. If you would like to load all of them at once, run the following command: <pre><code>source ~/scripting_workshop/modload.sh\n</code></pre></p> Output <pre><code>The following modules were not unloaded:\n(Use \"module --force purge\" to unload all):\n\n1) XALT/minimal   2) slurm   3) NeSI\nLoaded modules BWA, SAMtools, BCFtools,HISAT2,Subread\n</code></pre> <ul> <li>Please do not run <code>module --force purge</code> under any circumstances</li> </ul> <p>Indexing the genome</p> <pre><code>bwa index ref_genome/ecoli_rel606.fasta\n</code></pre> Output <pre><code>[bwa_index] Pack FASTA... 0.03 sec\n[bwa_index] Construct BWT for the packed sequence...\n[bwa_index] 1.04 seconds elapse.\n[bwa_index] Update BWT... 0.03 sec\n[bwa_index] Pack forward-only FASTA... 0.02 sec\n[bwa_index] Construct SA from BWT and Occ... 0.57 sec\n[main] Version: 0.7.17-r1188\n[main] CMD: bwa index ref_genome/ecoli_rel606.fasta\n[main] Real time: 2.462 sec; CPU: 1.702 sec\n</code></pre>"},{"location":"1_DesigningVariantC/#align-reads-to-reference-genome","title":"Align reads to reference genome","text":"<p>The alignment process consists of choosing an appropriate reference genome to map our reads against and then deciding on an aligner. We will use the BWA-MEM algorithm, which is the latest and is generally recommended for high-quality queries as it is faster and more accurate. We are going to start by aligning the reads from just one of the samples in our dataset (SRR2584866).</p> <p>script</p> <pre><code>bwa mem ref_genome/ecoli_rel606.fasta trimmed_reads/SRR2584866_1.trim.sub.fastq trimmed_reads/SRR2584866_2.trim.sub.fastq &gt; results/sam/SRR2584866.aligned.sam\n</code></pre> Output <pre><code>[M::bwa_idx_load_from_disk] read 0 ALT contigs\n[M::process] read 77446 sequences (10000033 bp)...\n[M::process] read 77296 sequences (10000182 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (48, 36728, 21, 61)\n[M::mem_pestat] analyzing insert size distribution for orientation FF...\n[M::mem_pestat] (25, 50, 75) percentile: (420, 660, 1774)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 4482)\n.....\n</code></pre> <p>script</p> <p><pre><code>ls results/sam/\n</code></pre> Output - <code>SRR2584866.aligned.sam</code> </p>"},{"location":"1_DesigningVariantC/#sambam-format","title":"SAM/BAM format","text":"<p>The SAM file is a tab-delimited text file that contains information for each individual read and it's alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification.</p> <p>The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file.</p> <p>We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (-S) and to output BAM format (-b):</p> <p>script</p> <p><pre><code>module load SAMtools/1.13-GCC-9.2.0\n</code></pre> <pre><code>samtools view -S -b results/sam/SRR2584866.aligned.sam &gt; results/bam/SRR2584866.aligned.bam\n</code></pre></p>"},{"location":"1_DesigningVariantC/#sort-bam-file-by-coordinates","title":"Sort BAM file by coordinates","text":"<p>Next we sort the BAM file using the <code>sort</code> command from samtools. The -o flag tells the command where to write the output.</p> <p>Terminal</p> <pre><code>samtools sort -o results/bam/SRR2584866.aligned.sorted.bam results/bam/SRR2584866.aligned.bam\n</code></pre> <p>Sorting</p> <p>SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input.</p> <p>You can use samtools to learn more about the bam file.</p> <pre><code>samtools flagstat results/bam/SRR2584866.aligned.sorted.bam\n</code></pre>"},{"location":"1_DesigningVariantC/#variant-calling","title":"Variant calling","text":"<p>A variant call is a conclusion that there is a nucleotide difference relative to a given reference at a given position in an individual genome or transcriptome, often referred to as a Single Nucleotide Variant (SNV). The call is usually accompanied by an estimate of variant frequency and some measure of confidence. Similar to other steps in this workflow, there are a number of tools available for variant calling. In this workshop we will be using <code>bcftools</code>, but there are a few things we need to do before actually calling the variants.</p>"},{"location":"1_DesigningVariantC/#step-1-calculate-the-read-coverage-of-positions-in-the-genome","title":"Step 1: Calculate the read coverage of positions in the genome","text":"<p>Do the first pass on variant calling by counting read coverage with <code>bcftools</code>. We will use the command mpileup. The flag -O b tells bcftools to generate a bcf format output file, -o specifies where to write the output file, and -f specifies the path to the reference genome:</p> <p>script</p> <p><pre><code>module load BCFtools/1.13-GCC-9.2.0\n</code></pre> <pre><code>bcftools mpileup -O b -o results/bcf/SRR2584866_raw.bcf -f ref_genome/ecoli_rel606.fasta results/bam/SRR2584866.aligned.sorted.bam\n</code></pre></p> Output <p>[mpileup] 1 samples in 1 input files [mpileup] maximum number of reads per input file set to -d 250</p> <p>We have now generated a file with coverage information for every base.</p>"},{"location":"1_DesigningVariantC/#step-2-detect-the-single-nucleotide-variants-snvs","title":"Step 2: Detect the single nucleotide variants (SNVs)","text":"<p>Identify SNVs using bcftools call. We have to specify ploidy with the flag <code>--ploidy</code>, which is one for the haploid E. coli. -m allows for multiallelic and rare-variant calling, -v tells the program to output variant sites only (not every site in the genome), and -o specifies where to write the output file:</p> <p>script</p> <pre><code>bcftools call --ploidy 1 -m -v -o results/vcf/SRR2584866_variants.vcf results/bcf/SRR2584866_raw.bcf \n</code></pre>"},{"location":"1_DesigningVariantC/#step-3-filter-and-report-the-snv-variants-in-variant-calling-format-vcf","title":"Step 3: Filter and report the SNV variants in variant calling format (VCF)","text":"<p>Filter the SNVs for the final output in VCF format, using <code>vcfutils.pl</code>:</p> <pre><code>vcfutils.pl varFilter results/vcf/SRR2584866_variants.vcf &gt; results/vcf/SRR2584866_final_variants.vcf\n</code></pre>"},{"location":"1_DesigningVariantC/#explore-the-vcf-format","title":"Explore the VCF format:","text":"<ul> <li>At this stage you can use various tools to analyse the vcf file. </li> <li>Exploring the vcf is beyond the scope of this workshop. Please refer to official documentation on VCF provided by Broad Institute</li> </ul> <p>Now we are ready for the Next Lesson to put all these commands in a script.</p>"},{"location":"2_AutomaticVariantC/","title":"Automating a Variant Calling Workflow","text":"<p>Aim</p> <ul> <li>Put all the steps from the previous lesson into a script.</li> </ul>"},{"location":"2_AutomaticVariantC/#variant-calling-workflow","title":"Variant calling workflow","text":"<p>Remember our variant calling workflow has the following steps:</p> <ul> <li>Index the reference genome for use by bwa and samtools.</li> <li>Align reads to reference genome.</li> <li>Convert the format of the alignment to sorted BAM, with some intermediate steps.</li> <li>Calculate the read coverage of positions in the genome.</li> <li>Detect the single nucleotide variants (SNVs).</li> <li>Filter and report the SNVs in VCF (variant calling format).</li> </ul> <p>Let's start with creating a new directory as our script working space and copy all the required resources.</p> <p><pre><code>pwd\n</code></pre> Output - <code>/home/$USER/scripting_workshop</code> <pre><code>mkdir script_workspace\n</code></pre> <pre><code>cd script_workspace\n</code></pre> <pre><code>#Don't forget the  . at the end of the line\ncp -r /nesi/project/nesi02659/scripting_workshop/variant_calling/* .\n</code></pre> <pre><code>ls\n</code></pre></p> <p>Output - <code>ref_genome  trimmed_reads</code></p> <p>Now we are ready to start building the script.</p> <p><pre><code>nano variant_calling.sh\n</code></pre> </p> Prefer Jupyter file over <code>nano</code> ? <p>You are welcome to choose Jupyter interactive file option over <code>nano</code>. If this is your preferred option, below are the instructions on how to create a file</p> <ol> <li>Make sure to navigate the left file explorer to correct path</li> <li>Right click on the explorer to open the menu and choose <code>New file</code></li> <li>Rename the file as <code>variant_calling.sh</code></li> </ol> <p></p> <p>In the text editor, type the commands</p> <p><pre><code>#!/bin/bash \n\n# Jane Doe\necho $(date)\n\n# This script runs the variant calling pipeline from mapping to vcf.\n\nset -e\n# Load all the required modules\nmodule purge\nmodule load BWA/0.7.17-GCC-9.2.0\nmodule load SAMtools/1.13-GCC-9.2.0\nmodule load BCFtools/1.13-GCC-9.2.0\n\n# create the results directories\nmkdir -p results/sam results/bam results/bcf results/vcf\n\n# indexing the genome\ngenome=ref_genome/ecoli_rel606.fasta\nbwa index $genome\n\n# create a loop that map reads to the genome, sort the bam files and call variants\nfor fq1 in trimmed_reads/*_1.trim.sub.fastq\n    do\n    echo \"working with file $fq1\"\n\n    base=$(basename $fq1 _1.trim.sub.fastq)\n    echo \"base name is $base\"\n\n   # setting the variables\n   fq1=trimmed_reads/${base}_1.trim.sub.fastq\n   fq2=trimmed_reads/${base}_2.trim.sub.fastq\n   sam=results/sam/${base}.aligned.sam\n   bam=results/bam/${base}.aligned.bam\n   sorted_bam=results/bam/${base}.aligned.sorted.bam\n   raw_bcf=results/bcf/${base}_raw.bcf\n   variants=results/vcf/${base}_variants.vcf\n   final_variants=results/vcf/${base}_final_variants.vcf\n\n  # running the analysis steps\n  bwa mem $genome $fq1 $fq2 &gt; $sam\n  samtools view -S -b $sam &gt; $bam\n  samtools sort -o $sorted_bam $bam\n  samtools index $sorted_bam\n  bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam\n  bcftools call --ploidy 1 -m -v -o $variants $raw_bcf\n  vcfutils.pl varFilter $variants &gt; $final_variants\n\ndone\n</code></pre> </p> Shell variables <p>A variable is a character string to which we assign a value. The value assigned could be a number, text, filename, device, or any other type of data. A variable is nothing more than a pointer to the actual data. The shell enables you to create, assign, and delete variables.</p> <p>Running the script (Running the script)</p> <pre><code>bash ./variant_calling.sh\n</code></pre> <p>Adding executable permissions</p> <p>The way the script is written means we have to indicate which program to use whenever we are running it.  To be able to run without calling bash, we need to change the script permissions.</p> <p>script</p> <pre><code>ls -l variant_calling.sh \n</code></pre> <p>Output <code>-rw-rw-r-- 1 fayfa80p fayfa80p 1401 Mar  5 22:29 variant_calling.sh</code></p> <p><pre><code>chmod u+x variant_calling.sh\n</code></pre> <pre><code>ls -l variant_calling.sh \n</code></pre></p> <p>Output - <code>-rwxrw-r-- 1 fayfa80p fayfa80p 1401 Mar  5 22:29 variant_calling.sh</code></p> <ul> <li>note colour change on the script filename</li> </ul> <p>Now we can execute the script without calling bash</p> <p>script</p> <pre><code>./variant_calling.sh\n</code></pre> <p>In the Next Lesson, we will  prepare the script to run on the HPC environment.</p>"},{"location":"3_RNAseq/","title":"RNA-Seq Mapping And Count Data Workflow","text":"<p>This material is extracted from the RNA-seq workshop Lesson</p> <p>Objectives and overall workflow</p> <ul> <li>To develop a pipeline that does mapping and count the number of reads that mapped overall, then put all these steps into a script.</li> <li>Understand and perform the steps involved in RNA-seq mapping and read count.</li> <li>Use command line tools to run the pipeline.</li> </ul> <p> </p>"},{"location":"3_RNAseq/#rna-seq-data-analysis-workflow","title":"RNA-Seq Data Analysis Workflow","text":""},{"location":"3_RNAseq/#assumptions","title":"Assumptions","text":"<ul> <li>You have already performed trimming and filtering of your reads and saved in a directory called trimmed_reads.</li> <li>You have a reference genome saved in a directory called ref_genome.</li> </ul> <p>In this workshop, we have already trimmed the reads and downloaded the reference genome for you. First, it is always good to verify where we are:</p> <p>script</p> <p><pre><code>cd ~/scripting_workshop\n</code></pre> <pre><code>pwd\n</code></pre></p> <p>Output - <code>/home/$USER/scripting_workshop</code></p> <p>Checking to make sure we have the directory and files for the workshop.</p> <p>script</p> <pre><code>ls -F\n</code></pre> <p>Output - <code>modload.sh*  rna_seq/  scheduler/  script_workspace/  variant_calling/</code></p> <p>Reminder</p> <p>If you do not have the workshop directory, you can copy it using the command: <code>cp -r  /nesi/project/nesi02659/scripting_workshop/ ~</code> </p> <p>script</p> <p><pre><code>cd rna_seq\n</code></pre> <pre><code>ls\n</code></pre> Output  - <code>ref_genome  trimmed_reads</code> </p>"},{"location":"3_RNAseq/#alignment-to-a-reference-genome","title":"Alignment to a reference genome","text":"<p>RNA-seq generate gene expression information by quantifying the number of transcripts (per gene) in a sample. This is acompished by counting the number of transcripts that have been sequenced - the more active a gene is, the more transcripts will be in a sample, and the more reads will be generated from that transcript.</p> <p>For RNA-seq, we need to align or map each read back to the genome, to see which gene produced it. - Highly expressed genes will generate lots of transcripts, so there will be lots of reads that map back to the position of that transcript in the genome. - The per-gene data we work with in an RNA-seq experiment are counts: the number of reads from each sample that originated from that gene.</p>"},{"location":"3_RNAseq/#preparation-of-the-genome","title":"Preparation of the genome","text":"<p>To be able to map (align) sequencing reads on the genome, the genome needs to be indexed first. In this workshop we will use HISAT2.</p> <p>script</p> <p><pre><code>cd ~/scripting_workshop/rna_seq/ref_genome\n</code></pre> <pre><code>#to list what is in your directory:\nls ~/scripting_workshop/rna_seq/ref_genome\n</code></pre> Output - <code>Saccharomyces_cerevisiae.R64-1-1.99.gtf  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa</code> <pre><code>module load HISAT2/2.2.0-gimkl-2020a\n</code></pre> <pre><code># index file:\nhisat2-build -p 4 -f Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa Saccharomyces_cerevisiae.R64-1-1.dna.toplevel\n</code></pre> <pre><code>#list what is in the directory:\nls ~/scripting_workshop/rna_seq/ref_genome\n</code></pre></p> Output <pre><code>Saccharomyces_cerevisiae.R64-1-1.99.gtf              Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.4.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.8.ht2\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.1.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.5.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.2.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.6.ht2\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.3.ht2  Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.7.ht2   \n</code></pre> <p>Arguments</p> <ul> <li>-p number of threads</li> <li>-f fasta file</li> </ul> <p>How many files were created during the indexing process?</p>"},{"location":"3_RNAseq/#alignment-on-the-genome","title":"Alignment on the genome","text":"<p>Now that the genome is prepared, sequencing reads can be aligned.</p> <p>Information required</p> <ul> <li>Where the sequence information is stored (e.g. fastq files ...) ?</li> <li>What kind of sequencing: Single End or Paired end ?</li> <li>Where are the indexes and the genome stored? </li> <li>Where will the mapping files be stored?</li> </ul> <ul> <li>Now, lets move one folder up (into the rna_seq folder):</li> </ul> <p>script</p> <p><pre><code>cd ..\n</code></pre> <pre><code>ls    \n</code></pre></p> <p>Let's map one of our sample to the reference genome:</p> <p>script</p> <p><pre><code>pwd\n</code></pre> Output    - <code>/home/$USER/scripting_workshop/rna_seq/</code></p> <p><pre><code>mkdir Mapping\n</code></pre> <pre><code>ls\n</code></pre> Output   - <code>ref_genome  Mapping  trimmed_reads</code></p> <p>Let's use a for loop to process our samples:</p> <p>script</p> <p><pre><code>cd trimmed_reads\n</code></pre> <pre><code>ls\n</code></pre> Output - <code>SRR014335-chr1.fastq  SRR014336-chr1.fastq  SRR014337-chr1.fastq  SRR014339-chr1.fastq  SRR014340-chr1.fastq  SRR014341-chr1.fastq</code> <pre><code>for filename in *\n do\n base=$(basename ${filename} .fastq)\n hisat2 -p 4 -x ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S ../Mapping/${base}.sam --summary-file ../Mapping/${base}_summary.txt\ndone\n</code></pre></p> <p>Arguments</p> <ul> <li>-x The basename of the index for the reference genome. </li> <li>-U Comma-separated list of files containing unpaired reads to be aligned</li> <li>-S File to write SAM alignments to. By default, alignments are written to the \u201cstandard out\u201d or \u201cstdout\u201d filehandle  </li> </ul> <p>Now we can explore our SAM files.</p> <p>script</p> <p><pre><code>cd ../Mapping\n</code></pre> <pre><code>ls\n</code></pre></p> Output <pre><code>SRR014335-chr1.sam          SRR014336-chr1_summary.txt  SRR014339-chr1.sam          SRR014340-chr1_summary.txt\nSRR014335-chr1_summary.txt  SRR014337-chr1.sam          SRR014339-chr1_summary.txt  SRR014341-chr1.sam\nSRR014336-chr1.sam          SRR014337-chr1_summary.txt  SRR014340-chr1.sam          SRR014341-chr1_summary.txt\n</code></pre>"},{"location":"3_RNAseq/#converting-sam-files-to-bam-files","title":"Converting SAM files to BAM files","text":"<p>The SAM file is a tab-delimited text file that contains information for each individual read and its alignment to the genome. While we do not have time to go into detail about the features of the SAM format, the paper by Heng Li et al. provides a lot more detail on the specification.</p> <p>The compressed binary version of SAM is called a BAM file. We use this version to reduce size and to allow for indexing, which enables efficient random access of the data contained within the file.</p>"},{"location":"3_RNAseq/#a-quick-look-into-the-sam-file","title":"A quick look into the sam file","text":"<p>script</p> <pre><code>less SRR014335-chr1.sam \n</code></pre> <p><code>.sam</code> header &amp; format</p> <p>The file begins with a header, which is optional. The header is used to describe the source of data, reference sequence, method of alignment, etc., this will change depending on the aligner being used. Following the header is the alignment section. Each line that follows corresponds to alignment information for a single read. Each alignment line has 11 mandatory fields for essential mapping information and a variable number of other fields for aligner specific information. An example entry from a SAM file is displayed below with the different fields highlighted.</p> <p>We will convert the SAM file to BAM format using the samtools program with the view command and tell this command that the input is in SAM format (<code>-S</code>) and to output BAM format (<code>-b</code>):</p> <p>script</p> <p><pre><code>module load SAMtools/1.13-GCC-9.2.0\n</code></pre> <pre><code>for filename in *.sam\n do\n base=$(basename ${filename} .sam)\n samtools view -S -b ${filename} -o ${base}.bam\ndone\n</code></pre> <pre><code>ls\n</code></pre></p> Output <pre><code>SRR014335-chr1.bam  SRR014336-chr1.bam  SRR014337-chr1.bam  SRR014339-chr1.bam  SRR014340-chr1.bam  SRR014341-chr1.bam\nSRR014335-chr1.sam  SRR014336-chr1.sam  SRR014337-chr1.sam  SRR014339-chr1.sam  SRR014340-chr1.sam  SRR014341-chr1.sam\n</code></pre> <p>Next we sort the BAM file using the sort command from samtools. <code>-o</code> tells the command where to write the output.</p> <p>Note</p> <p>SAM/BAM files can be sorted in multiple ways, e.g. by location of alignment on the chromosome, by read name, etc. It is important to be aware that different alignment tools will output differently sorted SAM/BAM, and different downstream tools require differently sorted alignment files as input.</p> <p>script</p> <pre><code>for filename in *.bam\n do\n base=$(basename ${filename} .bam)\n samtools sort -o ${base}_sorted.bam ${filename}\ndone\n</code></pre> <p>We can use samtools to learn more about the bam file as well.</p>"},{"location":"3_RNAseq/#some-stats-on-your-mapping","title":"Some stats on your mapping:","text":"<p>script</p> <pre><code>samtools flagstat SRR014335-chr1_sorted.bam \n</code></pre> Output <pre><code>156984 + 0 in total (QC-passed reads + QC-failed reads)\n31894 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n136447 + 0 mapped (86.92% : N/A)\n0 + 0 paired in sequencing\n0 + 0 read1\n0 + 0 read2\n0 + 0 properly paired (N/A : N/A)\n0 + 0 with itself and mate mapped\n0 + 0 singletons (N/A : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre>"},{"location":"3_RNAseq/#read-summarization","title":"Read Summarization","text":"<p>Sequencing reads often need to be assigned to genomic features of interest after they are mapped to the reference genome. This process is often called read summarization or read quantification. Read summarization is required by a number of downstream analyses such as gene expression analysis and histone modification analysis. The output of read summarization is a count table, in which the number of reads assigned to each feature in each library is recorded.</p>"},{"location":"3_RNAseq/#counting","title":"Counting","text":"<ul> <li>We need to do some counting!</li> <li>Want to generate count data for each gene (actually each exon) - how many reads mapped to each exon in the genome, from each of our samples?</li> <li>Once we have that information, we can start thinking about how to determine which genes were differentially expressed in our study.</li> </ul>"},{"location":"3_RNAseq/#subread-and-featurecounts","title":"Subread and FeatureCounts","text":"<ul> <li>The featureCounts tool from the Subread package can be used to count how many reads aligned to each genome feature (exon).</li> <li>We need to specify the annotation information (.gtf file).</li> </ul> <p>You can process all the samples at once:</p> <p>script</p> <p><pre><code>cd ~/scripting_workshop/rna_seq\n</code></pre> <pre><code>module load Subread/2.0.0-GCC-9.2.0\n</code></pre> <pre><code>pwd\n</code></pre> Output <code>/home/$USER/scripting_workshop/rna_seq</code> <pre><code>mkdir Counts\n</code></pre> <pre><code>cd Counts\n</code></pre> <pre><code>featureCounts -a ../ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o ./yeast_counts.txt -T 2 -t exon -g gene_id ../Mapping/*sorted.bam\n</code></pre></p> <p>Hint</p> <p>If you encounter an error along the lines of <code>ERROR: invalid parameter:..</code>, first check that your current working directory is <code>Counts</code>. (it's all about location, location, location \ud83d\ude09 )</p> <p>Arguments:</p> <ul> <li>-a Name of an annotation file. GTF/GFF format by default.</li> <li>-o Name of output file including read counts</li> <li>-T Specify the number of threads/CPUs used for mapping. 1 by default.</li> <li>-t Specify feature type in GTF annotation. 'exon' by default. Features used for read counting will be extracted from annotation using the provided value.</li> <li>-g Specify attribute type in GTF annotation. 'gene_id' by default. Meta-features used for read counting will be extracted from annotation using the provided value.</li> </ul> <p>Group Exercise</p> <p>Now, let's work together in our groups to create an RNA-seq mapping and count script.</p> <ul> <li>You will be assigned to breakout rooms</li> <li>One person from the room must volunteer to share their screen as the group works together to compile the script   </li> </ul> <ol> <li>It is safe to compile and submit the script from <code>/rna_seq</code> parent directory as it will override the existing results from the above steps. Applications used in this pipeline will obey the \"override\" by default. However, some applications will demand the existing outputs to be deleted or use a provided flag such as <code>--override</code> (if it is available as a function of the application)</li> <li>On the other hand, how about we bring more structure to \"results/outputs\" ? .i.e. Perhaps create a results directory for <code>sam</code>,<code>bam</code> and <code>counts</code> sub-directories than creating <code>/Mapping</code> (for both sam and bam) and <code>/Counts</code>  ? (Similar to what we have done in Variant calling pipeline)</li> <li>Also, the above testing was done with three separate <code>for</code> loops. Is it possible to bring them altogether under one <code>for</code> loop ?</li> </ol> <p>\ud83c\udf8a</p> <p>At this stage we have mastered the art of writing scripts, instead of running them on the command line, let us now run them on HPC.</p>"},{"location":"4_IntroductiontoHPC/","title":"Introduction to HPC","text":""},{"location":"4_IntroductiontoHPC/#defining-high-performance-computing","title":"Defining high-performance computing","text":"<p>The simplest way of defining high-performance computing is by saying that it is the use of high-performance computers (HPC). However, this leads to our next question: what is an HPC?</p> <p>A high-performance computer (HPC) is a network of computers in a cluster that typically share a common purpose and are used to accomplish tasks that might otherwise be too big for any one computer.</p> <p>While modern computers can do a lot (and a lot more than their equivalents 10-20 years ago), there are limits to what they can do and the speed at which they are able to do this. One way to overcome these limits is to pool computers together to create a cluster of computers. These pooled resources can then be used to run software that requires more total memory, or need more processors to complete in a reasonable time.</p> <p>One way to do this is to take a group of computers and link them together via a network switch. Consider a case where you have five 4-core computers. By connecting them together, you could run jobs on 20 cores, which could result in your software running faster.</p>"},{"location":"4_IntroductiontoHPC/#hpc-architectures","title":"HPC architectures","text":"<p>Most HPC systems follow the ideas described above of taking many computers and linking them via network switches.</p> <p>What distinguishes a high-performance computer from the computer clusters</p> <ul> <li>The number of computers/nodes </li> <li>The strength of each individual computer/node </li> <li>The network interconnect \u2013 this dictates the communication speed between nodes. The faster this speed is, the more a group of individual nodes will act like a unit.</li> </ul>"},{"location":"4_IntroductiontoHPC/#nesi-mahuika-cluster-architecture","title":"NeSI Mahuika Cluster architecture","text":"<p>NeSI Mahuika cluster (CRAY HPE CS400) system consists of a number of different node types. The ones visible to researchers are:</p> <ul> <li>Login nodes</li> <li>Compute nodes</li> </ul> Overview of HPC ArchitectureComposition of a nodeIn reality <p> </p> <p> </p> <p> </p> <p>Jupyter Terminal</p> <ul> <li>Jupyter terminal should be treated as a login node. .i.e. Just like what we have done so far; use it to develop, test and debug scripts but do not to deploy the production level workflow interactively.   </li> </ul> <p>Back to homepage</p>"},{"location":"5_working_with_job_scheduler/","title":"Working with job scheduler","text":""},{"location":"5_working_with_job_scheduler/#introduction-to-slurm-scheduler-and-directives","title":"Introduction to slurm scheduler and directives","text":"<p>An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when. In brief, scheduler is a </p> <ul> <li>Mechanism to control access by many users to shared computing resources</li> <li>Queuing / scheduling system for users\u2019 jobs</li> <li>Manages the reservation of resources and job execution on these resources </li> <li>Allows users to \u201cfire and forget\u201d large, long calculations or many jobs (\u201cproduction runs\u201d)</li> </ul> <p>A bit more on why do we need a scheduler ?</p> <ul> <li>To ensure the machine is utilised as fully as possible</li> <li>To ensure all users get a fair chance to use compute resources (demand usually exceeds supply)</li> <li>To track usage - for accounting and budget control</li> <li>To mediate access to other resources e.g. software licences</li> </ul> <p>Commonly used schedulers</p> <ul> <li> Slurm</li> <li>PBS , Torque</li> <li>Grid Engine</li> </ul> <p>All NeSI clusters use Slurm (Simple Linux Utility for Resource Management) scheduler (or job submission system) to manage resources and how they are made available to users.</p> <p></p> <p>Researchers can not communicate directly to  Compute nodes from the login node. Only way to establish a connection OR send scripts to compute nodes is to use scheduler as the carrier/manager</p>"},{"location":"5_working_with_job_scheduler/#life-cycle-of-a-slurm-job","title":"Life cycle of a slurm job","text":"<p>Commonly used Slurm commands</p> Command Function <code>sbatch</code> Submit non-interactive (batch) jobs to the scheduler <code>squeue</code> List jobs in the queue <code>scancel</code> Cancel a job <code>sacct</code> Display accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database <code>srun</code> Slurm directive for parallel computing <code>sinfo</code> Query the current state of nodes <code>salloc</code> Submit interactive jobs to the scheduler Exercise 5.1 (Optional) - Check the state of the compute cluster  <ul> <li> <p>summary of current states of compute nodes known to the scheduler <pre><code>sinfo\n</code></pre></p> </li> <li> <p>similar to above but expanded <pre><code>sinfo --format=\"%16P %.8m %.5a %10T %.5D %80N\"\n</code></pre></p> </li> <li> <p>will print a long output as it is one row per compute node in the cluster <pre><code>sinfo -N -l\n</code></pre></p> </li> <li> <p>Explore the capacity of a compute node  <pre><code>sinfo -n wch001 -o \"%n %c %m\"\n</code></pre></p> </li> </ul>"},{"location":"5_working_with_job_scheduler/#anatomy-of-a-slurm-script-and-submitting-first-slurm-job","title":"Anatomy of a slurm script and submitting first slurm job \ud83e\uddd0","text":"<p>As with most other scheduler systems, job submission scripts in Slurm consist of a header section with the shell specification and options to the submission command (<code>sbatch</code> in this case) followed by the body of the script that actually runs the commands you want. In the header section, options to <code>sbatch</code> should be prepended with <code>#SBATCH</code>.</p> <p> </p> <p>Commented lines <code>#</code></p> <p>Commented lines are ignored by the bash interpreter, but they are not ignored by slurm. The <code>#SBATCH</code> parameters are read by slurm when we submit the job. When the job starts, the bash interpreter will ignore all lines starting with <code>#</code>. This is very similar to the shebang mentioned earlier, when you run your script, the system looks at the <code>#!</code>, then uses the program at the subsequent path to interpret the script, in our case <code>/bin/bash</code> (the program <code>bash</code> found in the /bin directory</p> Slurm variables header use description --job-name <code>#SBATCH --job-name=MyJob</code> The name that will appear when using squeue or sacct. --account <code>#SBATCH --account=nesi12345</code> The account your core hours will be 'charged' to. --time <code>#SBATCH --time=DD-HH:MM:SS</code> Job max walltime. --mem <code>#SBATCH --mem=512MB</code> Memory required per node. --cpus-per-task <code>#SBATCH --cpus-per-task=10</code> Will request 10 logical CPUs per task. --output <code>#SBATCH --output=%j_output.out</code> Path and name of standard output file. <code>%j</code> will be replaced by the job ID. --mail-user <code>#SBATCH --mail-user=me23@gmail.com</code> address to send mail notifications. --mail-type <code>#SBATCH --mail-type=ALL</code> Will send a mail notification at BEGIN END FAIL. <code>#SBATCH --mail-type=TIME_LIMIT_80</code> Will send message at 80% walltime. <p></p> Assigning values to Slurm variables <p></p> <p></p> Exercise 5.2 <p>Let's put these directives together and compile our first slurm script. Below is a abstract version of the slurm life cycle to assist you with the process</p> <p></p> <ul> <li> <p>First step is to create new working directories inside the existing <code>~/scripting_workshop/scheduler</code> <pre><code> cd ~/scripting_workshop/scheduler\n</code></pre></p> </li> <li> <p>confirm the path is correct  <pre><code>pwd\n</code></pre></p> </li> <li> <p>create a new directory for this section and change the directory to it - Check for the follow up not <code>&amp;&amp;</code> <pre><code>mkdir ex_5.2 &amp;&amp; cd ex_5.2\n</code></pre></p> </li> </ul> <p>The meaning of <code>&amp;&amp;</code> and <code>&amp;</code> are intrinsically different.</p> <ul> <li>What is <code>&amp;&amp;</code> in Bash? In Bash\u2014and many other programming languages\u2014<code>&amp;&amp;</code> means \u201cAND\u201d. And in command execution context like this, it means items to the left as well as right of &amp;&amp; should be run in sequence in this case.</li> <li>What is &amp; in Bash? And a single <code>&amp;</code>means that the preceding commands\u2014to the immediate left of the &amp;\u2014should simply be run in the background.</li> </ul> <ul> <li> <p>use a text editor of choice to create a file named firstslurm.sl - we will use nano here <pre><code>nano firstslurm.sl\n</code></pre></p> </li> <li> <p>Content of <code>firstslurm.sl</code> should be as below. Please discuss as you make progress <pre><code>#!/bin/bash \n\n#SBATCH --job-name      myfirstslurmjob\n#SBATCH --account       nesi02659\n#SBATCH --time          00:02:00                 #Format is DD-HH:MM:SS\n#SBATCH --cpus-per-task 1\n#SBATCH --mem           512                      #Default unit is Megabytes\n#SBATCH --output        slurmjob.%j.out\n#SBATCH --error         slurmjob.%j.err\n\nsleep 100\n\necho \"I am a slurm job and I slept for 100 seconds\"\n\necho \"$SLURM_JOB_ID END\"\n</code></pre></p> </li> <li> <p>Save and Exit</p> </li> <li>Submit the script with <code>sbatch</code> command <pre><code>sbatch firstslurm.sl\n</code></pre></li> <li>Execute <code>squeue --me</code> and <code>sacct</code>. Discuss the outputs .i.e. <pre><code>squeue --me\n</code></pre> <pre><code>sacct\n</code></pre></li> </ul> <code>$SLURM_JOB_ID</code> <p><code>$SLURM_JOB_ID</code> is a Slurm environment variable.  - A full list of environment variables for SLURM can be found by visiting the SLURM page on environment variables - These variables are great for recursive operations. </p> <p>STDOUT/STDERR from jobs</p> <ul> <li>STDOUT - your process writes conventional output to this file handle</li> <li>STDERR - your process writes diagnostic output to this file handle.</li> </ul> <p>STDOUT and STDERR from jobs are, by default, written to a file called <code>slurm-JOBID.out</code> and <code>slurm-JOBID.err</code> in the working directory for the job (unless the job script changes this, this will be the directory where you submitted the job). So for a job with ID 12345 STDOUT and STDERR will be <code>slurm-12345.out</code> and <code>slurm-12345.err</code></p> <ul> <li>When things go wrong, first step of debugging (STORY TIME !) starts with a referral to these files. </li> </ul>"},{"location":"5_working_with_job_scheduler/#assessing-resource-utilisation-cpu-memory-time","title":"Assessing resource utilisation (cpu, memory, time)","text":"<p>Understanding the resources you have available and how to use them most efficiently is a vital skill in high performance computing. The three resources that every single job submitted on the platform needs to request are:</p> <ul> <li>CPUs (i.e. logical CPU cores), and</li> <li>Memory (RAM), and</li> <li>Time.</li> </ul> <p>What happens if I ask for the wrong resources?</p> Resource Asking for too much Not asking for enough Number of CPUs Job may wait in the queue for longer Job will run more slowly than expected, and so may run out time Drop in fairshare score which determines job priority Memory (above) Job will fail, probably with <code>OUT OF MEMORY</code> error, segmentation fault or bus error Wall time (above) Job will run out of time and get killed Exercise 5.3 <p>Let's submit another slurm job and review its resource utilisation</p> <ul> <li> <p>Change the working directory to Exercise_5.3 <pre><code>cd ~/scripting_workshop/scheduler/ex_5.3\n</code></pre></p> </li> <li> <p>Run <code>ls</code> command and you should see two files (one .R and one sl) and one directory named slurmout <pre><code>ls -F\n</code></pre> <pre><code>bowtie-test.sl*  input_data/  slurmout/\n</code></pre></p> </li> <li> <p>Review the slurm script bowtie-test.sl with nano and edit the corresponding sections (hint :email) <pre><code>sbatch bowtie-test.sl \n</code></pre></p> </li> <li> <p>use <code>squeue --me</code> and <code>sacct</code> again to evaluate the job status</p> </li> <li> <p>Once the job ran into completion, use <code>nn_seff JOBID</code> command to print the resource utilisation statistics (Replace JOBID with the corresponding number) <pre><code>$ nn_seff 25222190\nJob ID: 25222190\nCluster: mahuika\nUser/Group: me1234/me1234\nState: COMPLETED (exit code 0)\nCores: 1\nTasks: 1\nNodes: 1\nJob Wall-time:  18.33%  00:00:33 of 00:03:00 time limit\nCPU Efficiency: 93.94%  00:00:31 of 00:00:33 core-walltime\nMem Efficiency: 1.33%  13.62 MB of 1.00 GB\n</code></pre></p> </li> <li>Now review the content of <code>.err</code> and <code>.out</code> files in /slurmout directory</li> </ul> <p>Feeling adventurous \ud83e\udd20 ? - Refer to Supplementary material on slurm profiling</p>"},{"location":"5_working_with_job_scheduler/#compiling-slurm-scripts-for-variant-calling-and-rna-seq-episodes","title":"Compiling slurm scripts for Variant Calling and RNA-seq episodes","text":"Exercise 5.4 \ud83d\ude2c <p>Purpose of this exercise is to compile a slurm submission script based on the script we wrote in episode 2 - Automating variant calling workflow</p> <ul> <li>recommend creating a new directory for the exercise .i.e <code>ex_5.4</code></li> <li>Name of the file is <code>variant_calling.sl</code> (note that we have change the extension from <code>.sh</code> to <code>.sl</code>)</li> <li> <p>In terms of slurm variables</p> <ul> <li>name of the job is <code>variant_calling_workflow</code></li> <li>number of CPUS is <code>2</code></li> <li>timelimit <code>15 minutes</code></li> <li>amount of memory in GB <code>4G</code></li> <li>generate  .err files and .out where both should be re-directed to the directory slurmout</li> <li>an email notification at the end of the job </li> </ul> </li> <li> <p>We don't want to replicate input data  in multiple places .i.e. be conservative in-terms how you use research storage</p> </li> <li>Therefore, use the same reference genome file (assign the filename to variable <code>genome</code> and the trimmed read files (assign the path of these files to variable <code>trimmed</code>) used in the first episode <pre><code>genome=~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta\ntrimmed=~/scripting_workshop/variant_calling/trimmed_reads\n</code></pre></li> </ul> Exercise 5.5 \ud83d\ude2c <ul> <li>Now it's your turn to compile a slurm submission script for the RNA-seq workflow. \ud83d\ude0a</li> </ul> <p>Back to homepage</p>"},{"location":"6_supplementary_1/","title":"S1 : Accessing software via modules","text":"<p>On a high-performance computing system, it is quite rare that the software we want to use is available when we log in. It is installed, but we will need to \u201cload\u201d it before it can run.</p> <p>Before we start using individual software packages, however, we should understand the reasoning behind this approach. The three biggest factors are:</p> <ul> <li>software incompatibilities</li> <li>versioning</li> <li>dependencies</li> </ul> <p>One of the workarounds for this issue is Environment modules. A module is a self-contained description of a software package \u2014 it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages.</p> <p>There are a number of different environment module implementations commonly used on HPC systems and the one used in NeSI Mahuika cluster is <code>Lmod</code> where the <code>module</code> command is used to interact with environment modules.</p> <p>Commonly used <code>module</code> sub-commands</p> <ul> <li> <p>View available modules <pre><code>module avail\n</code></pre></p> </li> <li> <p>View all modules which match the keyword in their name <pre><code>module avail KEYWORD\n</code></pre></p> </li> <li> <p>View all modules which match the keyword in their name or description <pre><code>module spider KEYWORD\n</code></pre></p> </li> <li> <p>Load a specific program</p> <ul> <li>Note: All modules on NeSI have version and toolchain/environment suffixes. If none is specified, the default version for the tool is loaded. The default version can be seen with the module avail command. <pre><code>module load MY_APPLICATION\n</code></pre></li> </ul> </li> <li> <p>Swap a currently loaded module for a different one     <pre><code>module switch CURRENT_MODULE DESIRED_MODULE\n</code></pre></p> </li> <li> <p>Unload all current modules    <pre><code>module purge\n</code></pre></p> </li> </ul> <p>Danger</p> <p>Please do not use <code>$ module --force purge</code></p> <p>Back to homepage</p>"},{"location":"7_supplementary_2/","title":"S2 : slurm profiling","text":"<p>Although <code>nn_seff</code> command is a quick and easy way to determine the resource utilisation, it relies on peak values (data gets recorded every 30 seconds) which doesn't allows us to examine resource usage over the run-time of the job. There are number of in-built/external tools to achieve the latter which will require some effort to understand its deployment, tracing and interpretation. Therefore, we will use slurm native profiling to evaluate resource usage over run-time. This is a simple and elegant solution.</p> Exercise S.2.1 <ul> <li> <p>Download and decompress the content <pre><code>wget -c Exercise_S21.tar.gz https://github.com/DininduSenanayake/nesi_introductory_custom/releases/download/v1.0/Exercise_S21.tar.gz -O - | tar -xz\n</code></pre> <pre><code>cd Exercise_S21\n</code></pre></p> </li> <li> <p>Run <code>ls</code> command and you should see three files (one .R,sl and one .py - We will discuss the purpose of this .py file after submitting the job) and one directory named slurmout <pre><code>ls -F\n</code></pre> <pre><code>example1_arraysum.R  example1_arraysum.sl  profile_plot_Jul2020.py  slurmout/\n</code></pre></p> </li> <li> <p>Review the slurm script with cat Or another text editor and submit with sbatch <pre><code>sbatch example1_arraysum.sl \n</code></pre></p> </li> <li> <p>Do take a note of the JOBID as we are going to need it for next step. Otherwise, we use <code>squeue --me</code> OR <code>sacct</code> command as before to monitor the status</p> </li> <li>Also, you can <code>watch</code> the status of this job via <code>$ watch -n 1 -d \"squeue -j JOBID\"</code>. </li> <li> <p><code>watch</code> command execute a program periodically, showing output fullscreen. Exiting the <code>watch</code> screen by done by pressing <code>Ctrl+x</code> </p> </li> <li> <p>Let's create slurm profile graphs</p> <ul> <li> <p>collate the data into an HDF5 file using the command. Replace JOBID with the corresponding number  <pre><code>sh5util -j JOBID\n</code></pre> <pre><code>sh5util: Merging node-step files into ./job_JOBID.h5\n</code></pre></p> </li> <li> <p>execute the script on .h5 file. We will need one of the Python 3 modules to do this. Ignore the deprecating warning.  <pre><code>module purge \n</code></pre> <pre><code>module load Python/3.8.2-gimkl-2020a\n</code></pre></p> </li> <li>Replace JOBID with the corresponding number <pre><code>python profile_plot_Jul2020.py job_JOBID.h5\n</code></pre></li> </ul> </li> <li> <p>This should generate a .png file where the filename is in the format of job_23258404_profile.png</p> </li> </ul> <p> </p> <p>Back to homepage</p>"},{"location":"8_supplementary_3/","title":"S3 : Solutions","text":"RNA-Seq Mapping and Count Data \ud83d\ude3a <pre><code>#!/bin/bash -e\n\n# Jane Doe\n# 13 April 2023\n#This script uses relative paths. It has to be executed from rna_seq parent directory OR edit paths accordingly. \n\n# Load required modules and setup the environment\nmodule purge\nmodule load HISAT2/2.2.0-gimkl-2020a\nmodule load SAMtools/1.10-GCC-9.2.0\nmodule load Subread/2.0.0-GCC-9.2.0\n\n#Print the current working directory\necho \"${PWD}\"\n\n#Create results directories. We are moving away from \"Mapping\" and \"Counts\"\nmkdir -p results/{sam,bam,counts}\n\n# Index genome\nhisat2-build -p 2 -f ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa \\\n                     ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel\n\n# Align to indexed genome\nfor filename in trimmed_reads/*.fastq\ndo\n      # Extract base name\n      base=$(basename ${filename} .fastq)\n\n      # Align to the reference genome\n      hisat2 -p 2 -x ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename \\\n      -S results/sam/${base}.sam --summary-file results/sam/${base}.summary.txt\n\n      # Convert SAM to BAM\n      samtools view -S -b results/sam/${base}.sam | samtools sort -o results/bam/${base}_sorted.bam\n\n      # Extract stats for mapping\n      samtools flagstat results/bam/${base}_sorted.bam &gt; results/bam/${base}_mapstat.txt\ndone\n\n# count how many reads aligned to each genome feature (exon).\nfeatureCounts -a ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf \\\n              -o results/counts/yeast_counts.txt -T 2 -t exon -g gene_id \\\n                 results/bam/${base}_sorted.bam\n</code></pre> Exercise 5.4 \ud83d\ude0a <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      variant_calling_workflow\n#SBATCH --cpus-per-task 2\n#SBATCH --time          00:15:00\n#SBATCH --mem           4G\n#SBATCH --output        slurmout/variant_calling-%j.out\n#SBATCH --error         slurmout/variant_calling-%j.err\n#SBATCH --mail-type     END\n#SBATCH --mail-user     myemail@email.co.nz\n\n# Load all the required modules\nmodule purge\nmodule load BWA/0.7.17-GCC-9.2.0\nmodule load SAMtools/1.13-GCC-9.2.0\nmodule load BCFtools/1.13-GCC-9.2.0\n\necho \"$PWD\"\n\n# create the results directories\nmkdir -p results/sam results/bam results/bcf results/vcf\n\n# indexing the genome\ngenome=~/scripting_workshop/variant_calling/ref_genome/ecoli_rel606.fasta\ntrimmed=~/scripting_workshop/variant_calling/trimmed_reads\nbwa index $genome\n\n# create a loop that map reads to the genome, sort the bam files and call variants\nfor fq1 in ${trimmed}/*_1.trim.sub.fastq\n do\n    echo \"working with file $fq1\"\n\n    base=$(basename $fq1 _1.trim.sub.fastq)\n    echo \"base name is $base\"\n\n    # setting the variables\n    fq1=${trimmed}/${base}_1.trim.sub.fastq\n    fq2=${trimmed}/${base}_2.trim.sub.fastq\n    sam=results/sam/${base}.aligned.sam\n    bam=results/bam/${base}.aligned.bam\n    sorted_bam=results/bam/${base}.aligned.sorted.bam\n    raw_bcf=results/bcf/${base}_raw.bcf\n    variants=results/vcf/${base}_variants.vcf\n    final_variants=results/vcf/${base}_final_variants.vcf\n\n    # running the analysis steps\n    bwa mem $genome $fq1 $fq2 &gt; $sam\n    samtools view -S -b $sam &gt; $bam\n    samtools sort -o $sorted_bam $bam\n    samtools index $sorted_bam\n    bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam\n    bcftools call --ploidy 1 -m -v -o $variants $raw_bcf\n    vcfutils.pl varFilter $variants &gt; $final_variants\n\ndone\n\necho \"DONE\"\n</code></pre> Exercise 5.5 \ud83d\udc2e <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      rna-seq_workflow\n#SBATCH --cpus-per-task 2\n#SBATCH --time          00:15:00\n#SBATCH --mem           4G\n#SBATCH --output        rna-seq_workflow-%j.out\n#SBATCH --error         rna-seq_workflow-%j.err\n#SBATCH --mail-type     END\n#SBATCH --mail-user     myemail@email.org.nz\n\n\necho \"$PWD\"\n\nmkdir -p ~/scripting_workshop/scheduler/ex_5.5/{Mapping,Counts} &amp;&amp; cd ~/scripting_workshop/scheduler/ex_5.5/\ncp -r /nesi/project/nesi02659/scripting_workshop/rna_seq/* ./\n\nmodule purge\nmodule load HISAT2/2.2.0-gimkl-2020a\nmodule load SAMtools/1.10-GCC-9.2.0\nmodule load Subread/2.0.0-GCC-9.2.0\n\necho $PWD\n\n#index file\nhisat2-build -p 4 -f $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel\n\n\n#Mapping Samples to the reference genome\n\nfor filename in $PWD/trimmed_reads/*\n  do\n    base=$(basename ${filename} .fastq)\n    hisat2 -p 4 -x $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S $PWD/Mapping/${base}.sam --summary-file $PWD/Mapping/${base}_summary.txt\n  done\n\n#Convert SAMfiles to BAM\n\nfor filename in $PWD/Mapping/*.sam\n  do\n   base=$(basename ${filename} .sam)\n   samtools view -S -b ${filename} -o $PWD/Mapping/${base}.bam\n  done\n\n#Sort BAM files\n\nfor filename in $PWD/Mapping/*.bam\n  do\n    base=$(basename ${filename} .bam)\n    samtools sort -o $PWD/Mapping/${base}_sorted.bam ${filename}\n  done\n\n#count how many reads aligned to each genome feature (exon).\n\nfeatureCounts -a $PWD/ref_genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o $PWD/Counts/yeast_counts.txt -T 2 -t exon -g gene_id $PWD/Mapping/*sorted.bam\n</code></pre> <p>Back to homepage</p>"}]}